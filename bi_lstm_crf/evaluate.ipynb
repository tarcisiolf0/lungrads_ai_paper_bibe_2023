{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.lookup import KeyValueTensorInitializer, StaticHashTable\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, TimeDistributed, Dropout, Bidirectional, Dense, Layer, InputSpec\n",
    "from tensorflow_addons.text import crf_log_likelihood, viterbi_decode, crf_decode\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.scheme import IOB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(input_dim, output_dim, input_length, mask_zero):\n",
    "    return Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length, mask_zero = mask_zero)\n",
    "\n",
    "def bilstm_crf(maxlen, n_tags, lstm_units, embedding_dim, n_words, mask_zero, training = True):\n",
    "    \"\"\"\n",
    "    bilstm_crf - module to build BiLSTM-CRF model\n",
    "    Inputs:\n",
    "        - input_shape : tuple\n",
    "            Tensor shape of inputs, excluding batch size\n",
    "    Outputs:\n",
    "        - output : tensorflow.keras.outputs.output\n",
    "            BiLSTM-CRF output\n",
    "    \"\"\"\n",
    "    input = Input(shape = (maxlen,))\n",
    "    # Embedding layer\n",
    "    embeddings = embedding_layer(input_dim = n_words, output_dim = embedding_dim, input_length = maxlen, mask_zero = mask_zero)\n",
    "    output = embeddings(input)\n",
    "\n",
    "    # BiLSTM layer\n",
    "    output = Bidirectional(LSTM(units = lstm_units, return_sequences = True, recurrent_dropout = 0.1))(output)\n",
    "\n",
    "    # Dense layer\n",
    "    output = TimeDistributed(Dense(n_tags, activation = 'relu'))(output)\n",
    "\n",
    "    output = CRF(n_tags, name = 'crf_layer')(output)\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sparse_target=True,\n",
    "                 transitions=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_dim (int): the number of labels to tag each temporal input.\n",
    "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
    "        Input shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        Output shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.output_dim = int(output_dim)\n",
    "        self.sparse_target = sparse_target\n",
    "        self.input_spec = InputSpec(min_ndim=3)\n",
    "        self.supports_masking = False\n",
    "        self.sequence_lengths = None\n",
    "        self.transitions = transitions\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        f_shape = tf.TensorShape(input_shape)\n",
    "        input_spec = InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
    "\n",
    "        if f_shape[-1] is None:\n",
    "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        if f_shape[-1] != self.output_dim:\n",
    "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
    "                             ' shape. Use a linear layer if needed.')\n",
    "        self.input_spec = input_spec\n",
    "        self.transitions = self.add_weight(name='transitions',\n",
    "                                           shape=[self.output_dim, self.output_dim],\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Just pass the received mask from previous layer, to the next layer or\n",
    "        # manipulate it if this layer changes the shape of the input\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
    "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        if sequence_lengths is not None:\n",
    "            assert len(sequence_lengths.shape) == 2\n",
    "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
    "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
    "            assert seq_len_shape[1] == 1\n",
    "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
    "        else:\n",
    "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
    "                tf.shape(inputs)[1]\n",
    "            )\n",
    "\n",
    "        viterbi_sequence, _ = crf_decode(sequences,\n",
    "                                         self.transitions,\n",
    "                                         self.sequence_lengths)\n",
    "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
    "        return K.in_train_phase(sequences, output)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        def crf_loss(y_true, y_pred):\n",
    "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "            log_likelihood, self.transitions = crf_log_likelihood(\n",
    "                y_pred,\n",
    "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "                self.sequence_lengths,\n",
    "                transition_params=self.transitions,\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        return crf_loss\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        def viterbi_accuracy(y_true, y_pred):\n",
    "            # -1e10 to avoid zero at sum(mask)\n",
    "            mask = K.cast(\n",
    "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            if self.sparse_target:\n",
    "                y_true = K.argmax(y_true, 2)\n",
    "            y_pred = K.cast(y_pred, 'int32')\n",
    "            y_true = K.cast(y_true, 'int32')\n",
    "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "            return K.sum(corrects * mask) / K.sum(mask)\n",
    "        return viterbi_accuracy\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
    "        return input_shape[:2] + (self.output_dim,)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CRF, self).get_config()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "            'sparse_target': self.sparse_target,\n",
    "            'transitions': self.transitions.numpy()  # Convert the transitions to a NumPy array\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Since 'transitions' is a NumPy array, we need to convert it back to a tensor\n",
    "        transitions = tf.convert_to_tensor(config['transitions'])\n",
    "        # Create a new instance of CRF with the saved configuration\n",
    "        return cls(output_dim=config['output_dim'], sparse_target=config['sparse_target'], transitions=transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def viterbi_accuracy(y_true, y_pred):\n",
    "    mask = K.cast(K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "    shape = tf.shape(y_pred)\n",
    "    sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "    y_pred, _ = crf_decode(y_pred, K.zeros_like(y_pred), sequence_lengths)\n",
    "    if K.ndim(y_true) == K.ndim(y_pred) - 1:\n",
    "        y_true = K.expand_dims(y_true, K.ndim(y_pred) - 1)\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "    return K.sum(corrects * mask) / K.sum(mask)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_accuracy(y_true, y_pred):\n",
    "    # -1e10 to avoid zero at sum(mask)\n",
    "    mask = K.cast(\n",
    "        K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "    shape = tf.shape(y_pred)\n",
    "    sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "    y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "    if self.sparse_target:\n",
    "        y_true = K.argmax(y_true, 2)\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "    return K.sum(corrects * mask) / K.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def crf_loss(y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=CRF(dtype='float32').dtype)\n",
    "    log_likelihood, _ = tf.keras.layers.CRF(dtype='float32')(y_pred, y_true)\n",
    "    return tf.reduce_mean(-log_likelihood)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_loss(y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "    log_likelihood, self.transitions = crf_log_likelihood(\n",
    "        y_pred,\n",
    "        tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "        self.sequence_lengths,\n",
    "        transition_params=self.transitions,\n",
    "    )\n",
    "    return tf.reduce_mean(-log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded_model = tf.keras.models.load_model('model_teste_load', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"word\"].values.tolist(),\n",
    "                  data[\"tag\"].values.tolist())\n",
    "    return [(word, tag) for word, tag in iterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "  all_words = list(set(data[\"word\"].values))\n",
    "  all_tags = list(set(data[\"tag\"].values))\n",
    "\n",
    "  word2index = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
    "\n",
    "  word2index[\"--UNKNOWN_WORD--\"] = 0\n",
    "\n",
    "  word2index[\"--PADDING--\"] = 1\n",
    "\n",
    "  index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "  tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "  tag2index[\"--PADDING--\"] = 0\n",
    "\n",
    "  index2tag = {idx: word for word, idx in tag2index.items()}\n",
    "\n",
    "  return word2index, index2word, tag2index, index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(reports, word2index, tag2index):\n",
    "  contents = []\n",
    "  labels = []\n",
    "  for report in reports:\n",
    "    content = []\n",
    "    label = []\n",
    "    for i in range(len(report)):\n",
    "      word, tag = report[i]\n",
    "      word_idx = word2index.get(word, 0)\n",
    "      tag_idx = tag2index.get(tag, 0)\n",
    "      content.append(word_idx)\n",
    "      label.append(tag_idx)\n",
    "\n",
    "    contents.append(content)\n",
    "    labels.append(label)\n",
    "\n",
    "  \"\"\"\n",
    "  padding the array with max_sentence_size\n",
    "  pad_sequences(sequences, maxlen=None, dtype=\"int32\", padding=\"pre\", truncating=\"pre\", value=0.0,):\n",
    "  the maxlen argument if provided, or the length of the longest sequence in the list.\n",
    "  \"\"\"\n",
    "\n",
    "  max_sentence_size = 512\n",
    "  contents = tf.keras.preprocessing.sequence.pad_sequences(contents, maxlen=max_sentence_size, padding='post', value=1)\n",
    "  labels = tf.keras.preprocessing.sequence.pad_sequences(labels, maxlen=max_sentence_size, padding='post')\n",
    "\n",
    "  #max_sentence = len(contents[0])\n",
    "  tag_size = len(tag2index)\n",
    "\n",
    "  #print(max_sentence)\n",
    "\n",
    "  labels_categorical = [tf.keras.utils.to_categorical(i, num_classes=tag_size) for i in labels]\n",
    "  labels_categorical = np.asarray(labels_categorical)\n",
    "\n",
    "  return contents, labels, labels_categorical, max_sentence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_word_test_sentences_and_tags(index2tag, index2word, X_test, y_test):\n",
    "\n",
    "    test_sentences= []\n",
    "    test_tags = []\n",
    "\n",
    "    # Recupera os laudos e tags no formato word2index/tag2index\n",
    "    for i in range(len(X_test)):\n",
    "        aux_tag = []\n",
    "\n",
    "        report = \"\"\n",
    "        sentence = X_test[i]\n",
    "        tags = y_test[i]\n",
    "\n",
    "        # Recupera o laudo\n",
    "        for j in range(len(sentence)):\n",
    "            # Recupera a palavra\n",
    "            word = sentence[j]\n",
    "            # Recupera a tag\n",
    "            tag = tags[j]\n",
    "            \"\"\"\n",
    "            print(word)\n",
    "            print(tag)\n",
    "            \"\"\"\n",
    "            # A tag é predita em one-hot-enconding\n",
    "            # int_tag é o inteiro que representa a tag\n",
    "            # no dicionário index2tag\n",
    "            int_tag = np.where(tag == int(1))\n",
    "            \"\"\"\n",
    "            print(int_tag)\n",
    "            \"\"\"\n",
    "            # Constrói o laudo ignorando as palavras \"padding\"\n",
    "            # Constrói o array de tags do laudo\n",
    "            if str(index2word[word]) != '--PADDING--':\n",
    "                report = report + \" \" + str(index2word[word])\n",
    "                aux_tag.append(index2tag[int(int_tag[0][0])])\n",
    "\n",
    "        \"\"\"\n",
    "        print(report)\n",
    "        print(aux_tag)\n",
    "        \"\"\"\n",
    "        test_sentences.append(report)\n",
    "        test_tags.append(aux_tag)\n",
    "\n",
    "    return test_sentences, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_df_model_previous(test_sentences, test_tags, model, word2index, index2tag, MAX_SENTENCE):\n",
    "\n",
    "    test_df = pd.DataFrame(columns = ['report', 'word', 'tag', 'tag_pred'])\n",
    "\n",
    "    for i in range (len(test_sentences)):\n",
    "\n",
    "        # Gera os laudos no formato index2word com o tamanho max_sentence\n",
    "\n",
    "        #print(\"LAUDO \" + str(i) + \"____________________________________________________________________________________________\")\n",
    "        sentence = test_sentences[i]\n",
    "        tags = test_tags[i]\n",
    "        \n",
    "        sentence = sentence.split()\n",
    "        padded_sentence = sentence + [word2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(sentence))\n",
    "        padded_sentence = [word2index.get(w, 0) for w in padded_sentence]\n",
    "\n",
    "        # Faz a predição das tags das palavras\n",
    "        pred = model.predict(np.array([padded_sentence]))\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        #print(len(padded_sentence))\n",
    "        #print(len(tags))\n",
    "        #print(len(pred[0]))\n",
    "        #print(pred[0])\n",
    "\n",
    "        if i < 10:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "                #test_df = test_df.append({'sentence': ('sentence_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}, ignore_index = True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "                #test_df = test_df.append({'sentence': ('sentence_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}, ignore_index = True)\n",
    "\n",
    "        #print(retval)\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def result_df_model_previous(test_sentences_int ,test_sentences, test_tags, model, word2index, index2tag, MAX_SENTENCE):\n",
    "\n",
    "    test_df = pd.DataFrame(columns = ['report', 'word', 'tag', 'tag_pred'])\n",
    "\n",
    "    for i in range (len(test_sentences)):\n",
    "\n",
    "        # Gera os laudos no formato index2word com o tamanho max_sentence\n",
    "\n",
    "        #print(\"LAUDO \" + str(i) + \"____________________________________________________________________________________________\")\n",
    "        sentence = test_sentences[i]\n",
    "        tags = test_tags[i]\n",
    "        \n",
    "        sentence = sentence.split()\n",
    "        #padded_sentence = sentence + [word2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(sentence))\n",
    "        #padded_sentence = [word2index.get(w, 0) for w in padded_sentence]\n",
    "\n",
    "        # Faz a predição das tags das palavras\n",
    "        #pred = model.predict(np.array([padded_sentence]))\n",
    "        sentence_int = test_sentences_int[i]\n",
    "        pred = model.predict(sentence_int )\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        if i < 10:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "\n",
    "\n",
    "        else:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "\n",
    "        #print(retval)\n",
    "\n",
    "    return test_df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('laudos_1_963_iob.csv', encoding= 'utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('test_mod_df.csv', encoding= 'utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, index2word, tag2index, index2tag = build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = data_test.groupby(\"report\").apply(to_tuples).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_sequences, test_tag_sequences, test_tag_sequences_categorical, max_len = tokenize(reports, word2index, tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4007, 3036, 4096, ...,    1,    1,    1],\n",
       "       [4007, 3036, 4096, ...,    1,    1,    1],\n",
       "       [4007, 3036, 4096, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [4007, 3036, 4096, ...,    1,    1,    1],\n",
       "       [2775, 3015, 2267, ...,    1,    1,    1],\n",
       "       [4007, 3036, 4096, ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences, test_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, test_text_sequences, test_tag_sequences_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4007, 3036, 4096, 2012, 1159, 2976,  771, 1459,  659, 3915, 2027,\n",
       "       3622,  111, 4062, 1851, 1131,  526, 2329,  439, 2329, 1562,  864,\n",
       "        815, 2148, 2267, 2291,  659,   95,  145, 1691, 3030, 2618, 1497,\n",
       "         26, 3622, 4098,  824,  577, 2595, 1949,  119, 1331, 2267, 3217,\n",
       "       2631, 1551, 1286, 3658, 3397, 1286, 3897,  779, 1286, 1537, 1691,\n",
       "       4098,  824, 2595, 1949,  119, 1331, 2267, 3217, 2631, 3672, 1286,\n",
       "       2601, 1691, 1467, 3389, 1694, 1427,  771, 3562, 1960, 1286, 1165,\n",
       "       1286, 1694, 3389, 3343, 1765, 1286, 2495, 3134, 3776, 1723, 1286,\n",
       "       2484, 1537, 1286, 1243, 1054,  659, 2000, 2733, 1286,  577, 1243,\n",
       "       1289, 3931, 1851, 1623, 1286,  903, 1286, 1727,  709,  771, 3305,\n",
       "       3220, 1691, 3820, 3296,  659, 2847, 2478,  216, 1286, 1474, 2876,\n",
       "        864, 2161, 1691, 2725, 2156, 2078, 2714, 1691,  298, 1792, 3622,\n",
       "       2440, 2368,  574, 1949, 3217,  887, 1551, 1286, 3658, 3891, 1286,\n",
       "       3897,  779, 1286, 3380,  705, 2732, 1562,   95, 1691, 2440, 2368,\n",
       "        574, 1949, 1571, 3672, 1286,  659,  305, 1955, 1286,  446, 3380,\n",
       "        705, 2732, 1562,   95, 1286, 3802, 2973, 1949, 2732, 1691, 3391,\n",
       "       1882, 1286,  864, 2415, 3622, 2400, 1117, 1896, 1694, 1398,  659,\n",
       "       1427, 1514, 1691, 2146, 2618, 2683, 3239, 1694,  823, 2618, 3630,\n",
       "       1514, 1691, 1033, 1651, 1661, 1691, 2281, 3213, 1424,   78, 1184,\n",
       "       1286, 3847, 2249, 2372, 1691, 2141,  545, 2267,  336, 1694, 3012,\n",
       "       4093, 3590, 1691, 3605, 1063, 1730, 2754,   45, 1691, 2440,  788,\n",
       "       1694,   32,  634, 1691, 3391, 3147, 1184,  634, 1691,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'I-Tamanho',\n",
       " 2: 'B-Localização',\n",
       " 3: 'B-Calcificação',\n",
       " 4: 'I-Bordas',\n",
       " 5: 'B-Bordas',\n",
       " 6: 'B-Achado',\n",
       " 7: 'O',\n",
       " 8: 'I-Calcificação',\n",
       " 9: 'B-Atenuação',\n",
       " 10: 'I-Localização',\n",
       " 11: 'I-Achado',\n",
       " 12: 'B-Tamanho',\n",
       " 13: 'I-Atenuação',\n",
       " 0: '--PADDING--'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"word2index, index2word, tag2index, index2tag = build_vocab(data)\n",
    "reports = data.groupby(\"report\").apply(to_tuples).tolist()\n",
    "text_sequences, tag_sequences, tag_sequences_categorical, max_len = tokenize(reports, word2index, tag2index)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_sequences, tag_sequences_categorical, test_size=0.1, random_state=1234)\n",
    "test_sentences, test_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, X_test, y_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tarci\\OneDrive\\Área de Trabalho\\mestrado\\bilstm_antigo\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = cur_dir+'\\\\models\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#loaded_model_00 = tf.keras.models.load_model(models_dir+'model_00', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loaded_model_00 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(models_dir\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodel_00\u001b[39;49m\u001b[39m'\u001b[39;49m, custom_objects\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mCRF\u001b[39;49m\u001b[39m'\u001b[39;49m: CRF})\n\u001b[0;32m      4\u001b[0m result_df_model_00 \u001b[39m=\u001b[39m result_df_model_previous(test_sentences, test_tags, loaded_model_00, word2index, index2tag, max_len)\n\u001b[0;32m      5\u001b[0m result_df_model_00\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mresult_df_model_00.csv\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\saving\\saved_model\\load.py:1137\u001b[0m, in \u001b[0;36mrevive_custom_object\u001b[1;34m(identifier, metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m revived_cls\u001b[39m.\u001b[39m_init_from_metadata(metadata)\n\u001b[0;32m   1136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to restore custom object of type \u001b[39m\u001b[39m{\u001b[39;00midentifier\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease make sure that any custom layers are included in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`custom_objects` arg when calling `load_model()` and make sure \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat all layers implement `get_config` and `from_config`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1142\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`."
     ]
    }
   ],
   "source": [
    "loaded_model_00 = tf.keras.models.load_model(models_dir+'model_00', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "result_df_model_00 = result_df_model_previous(test_sentences, test_tags, loaded_model_00, word2index, index2tag, max_len)\n",
    "result_df_model_00.to_csv(\"result_df_model_00.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_00 = tf.keras.models.load_model(models_dir+'model_00', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_01 = tf.keras.models.load_model(models_dir+'model_01', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_02 = tf.keras.models.load_model(models_dir+'model_02', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_03 = tf.keras.models.load_model(models_dir+'model_03', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_04 = tf.keras.models.load_model(models_dir+'model_04', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_05 = tf.keras.models.load_model(models_dir+'model_05', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_06 = tf.keras.models.load_model(models_dir+'model_06', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_07 = tf.keras.models.load_model(models_dir+'model_07', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_08 = tf.keras.models.load_model(models_dir+'model_08', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_09 = tf.keras.models.load_model(models_dir+'model_09', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_10 = tf.keras.models.load_model(models_dir+'model_10', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_11 = tf.keras.models.load_model(models_dir+'model_11', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_12 = tf.keras.models.load_model(models_dir+'model_12', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_13 = tf.keras.models.load_model(models_dir+'model_13', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_14 = tf.keras.models.load_model(models_dir+'model_14', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_15 = tf.keras.models.load_model(models_dir+'model_15', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_16 = tf.keras.models.load_model(models_dir+'model_16', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_17 = tf.keras.models.load_model(models_dir+'model_17', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_18 = tf.keras.models.load_model(models_dir+'model_18', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_19 = tf.keras.models.load_model(models_dir+'model_19', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_20 = tf.keras.models.load_model(models_dir+'model_20', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_21 = tf.keras.models.load_model(models_dir+'model_21', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_22 = tf.keras.models.load_model(models_dir+'model_22', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_23 = tf.keras.models.load_model(models_dir+'model_23', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_24 = tf.keras.models.load_model(models_dir+'model_24', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_25 = tf.keras.models.load_model(models_dir+'model_25', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n",
    "loaded_model_26 = tf.keras.models.load_model(models_dir+'model_26', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m result_df_model_00 \u001b[39m=\u001b[39m result_df_model_previous(test_sentences, test_tags, loaded_model_00, word2index, index2tag, max_len)\n\u001b[0;32m      2\u001b[0m result_df_model_00\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mresult_df_model_00.csv\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m result_df_model_01 \u001b[39m=\u001b[39m result_df_model_previous(test_sentences, test_tags, loaded_model_01, word2index, index2tag, max_len)\n\u001b[0;32m      5\u001b[0m result_df_model_01\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mresult_df_model_01.csv\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m result_df_model_02 \u001b[39m=\u001b[39m result_df_model_previous(test_sentences, test_tags, loaded_model_02, word2index, index2tag, max_len)\n",
      "Cell \u001b[1;32mIn[38], line 42\u001b[0m, in \u001b[0;36mresult_df_model_previous\u001b[1;34m(test_sentences, test_tags, model, word2index, index2tag, MAX_SENTENCE)\u001b[0m\n\u001b[0;32m     40\u001b[0m             aux_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mreport\u001b[39m\u001b[39m'\u001b[39m: (\u001b[39m'\u001b[39m\u001b[39mreport_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i)), \u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m: w, \u001b[39m'\u001b[39m\u001b[39mtag\u001b[39m\u001b[39m'\u001b[39m : t, \u001b[39m'\u001b[39m\u001b[39mtag_pred\u001b[39m\u001b[39m'\u001b[39m : index2tag[p]}\n\u001b[0;32m     41\u001b[0m             df_new_row \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([aux_dict])\n\u001b[1;32m---> 42\u001b[0m             test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([test_df, df_new_row])\n\u001b[0;32m     43\u001b[0m             \u001b[39m#test_df = test_df.append({'sentence': ('sentence_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}, ignore_index = True)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m     \u001b[39m#print(retval)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m test_df\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[0;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    159\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m         objs,\n\u001b[0;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    379\u001b[0m     )\n\u001b[0;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:563\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[0;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[1;32m--> 563\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_new_axes()\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:633\u001b[0m, in \u001b[0;36m_Concatenator._get_new_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[0;32m    632\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[1;32m--> 633\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[0;32m    635\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[0;32m    636\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:634\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[0;32m    632\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 634\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[0;32m    635\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[0;32m    636\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:691\u001b[0m, in \u001b[0;36m_Concatenator._get_concat_axis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mlevels supported only when keys is not None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 691\u001b[0m     concat_axis \u001b[39m=\u001b[39m _concat_indexes(indexes)\n\u001b[0;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    693\u001b[0m     concat_axis \u001b[39m=\u001b[39m _make_concat_multiindex(\n\u001b[0;32m    694\u001b[0m         indexes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames\n\u001b[0;32m    695\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:709\u001b[0m, in \u001b[0;36m_concat_indexes\u001b[1;34m(indexes)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_concat_indexes\u001b[39m(indexes) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[1;32m--> 709\u001b[0m     \u001b[39mreturn\u001b[39;00m indexes[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mappend(indexes[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5405\u001b[0m, in \u001b[0;36mIndex.append\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   5402\u001b[0m names \u001b[39m=\u001b[39m {obj\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m to_concat}\n\u001b[0;32m   5403\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(names) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n\u001b[1;32m-> 5405\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concat(to_concat, name)\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5413\u001b[0m, in \u001b[0;36mIndex._concat\u001b[1;34m(self, to_concat, name)\u001b[0m\n\u001b[0;32m   5408\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5409\u001b[0m \u001b[39mConcatenate multiple Index objects.\u001b[39;00m\n\u001b[0;32m   5410\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5411\u001b[0m to_concat_vals \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39m_values \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[1;32m-> 5413\u001b[0m result \u001b[39m=\u001b[39m concat_compat(to_concat_vals)\n\u001b[0;32m   5415\u001b[0m is_numeric \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   5416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_backward_compat_public_numeric_index \u001b[39mand\u001b[39;00m is_numeric:\n",
      "File \u001b[1;32mc:\\Users\\tarci\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    148\u001b[0m             to_concat \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[0;32m    149\u001b[0m             kinds \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m--> 151\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(to_concat, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kinds \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    153\u001b[0m     \u001b[39m# GH#39817\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBehavior when concatenating bool-dtype and numeric-dtype arrays is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated; in a future version these will cast to object dtype \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    161\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_df_model_00 = result_df_model_previous(test_sentences, test_tags, loaded_model_00, word2index, index2tag, max_len)\n",
    "result_df_model_00.to_csv(\"result_df_model_00.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_01 = result_df_model_previous(test_sentences, test_tags, loaded_model_01, word2index, index2tag, max_len)\n",
    "result_df_model_01.to_csv(\"result_df_model_01.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_02 = result_df_model_previous(test_sentences, test_tags, loaded_model_02, word2index, index2tag, max_len)\n",
    "result_df_model_02.to_csv(\"result_df_model_02.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_03 = result_df_model_previous(test_sentences, test_tags, loaded_model_03, word2index, index2tag, max_len)\n",
    "result_df_model_03.to_csv(\"result_df_model_03.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_04 = result_df_model_previous(test_sentences, test_tags, loaded_model_04, word2index, index2tag, max_len)\n",
    "result_df_model_04.to_csv(\"result_df_model_04.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_05 = result_df_model_previous(test_sentences, test_tags, loaded_model_05, word2index, index2tag, max_len)\n",
    "result_df_model_05.to_csv(\"result_df_model_05.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_06 = result_df_model_previous(test_sentences, test_tags, loaded_model_06, word2index, index2tag, max_len)\n",
    "result_df_model_06.to_csv(\"result_df_model_06.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_07 = result_df_model_previous(test_sentences, test_tags, loaded_model_07, word2index, index2tag, max_len)\n",
    "result_df_model_07.to_csv(\"result_df_model_07.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_08 = result_df_model_previous(test_sentences, test_tags, loaded_model_08, word2index, index2tag, max_len)\n",
    "result_df_model_08.to_csv(\"result_df_model_08.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_09 = result_df_model_previous(test_sentences, test_tags, loaded_model_09, word2index, index2tag, max_len)\n",
    "result_df_model_09.to_csv(\"result_df_model_09.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_10 = result_df_model_previous(test_sentences, test_tags, loaded_model_10, word2index, index2tag, max_len)\n",
    "result_df_model_10.to_csv(\"result_df_model_10.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_11 = result_df_model_previous(test_sentences, test_tags, loaded_model_11, word2index, index2tag, max_len)\n",
    "result_df_model_11.to_csv(\"result_df_model_11.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_12 = result_df_model_previous(test_sentences, test_tags, loaded_model_12, word2index, index2tag, max_len)\n",
    "result_df_model_12.to_csv(\"result_df_model_12.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_13 = result_df_model_previous(test_sentences, test_tags, loaded_model_13, word2index, index2tag, max_len)\n",
    "result_df_model_13.to_csv(\"result_df_model_13.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_14 = result_df_model_previous(test_sentences, test_tags, loaded_model_14, word2index, index2tag, max_len)\n",
    "result_df_model_14.to_csv(\"result_df_model_14.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_15 = result_df_model_previous(test_sentences, test_tags, loaded_model_15, word2index, index2tag, max_len)\n",
    "result_df_model_15.to_csv(\"result_df_model_15.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_16 = result_df_model_previous(test_sentences, test_tags, loaded_model_16, word2index, index2tag, max_len)\n",
    "result_df_model_16.to_csv(\"result_df_model_16.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_17 = result_df_model_previous(test_sentences, test_tags, loaded_model_17, word2index, index2tag, max_len)\n",
    "result_df_model_17.to_csv(\"result_df_model_17.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_18 = result_df_model_previous(test_sentences, test_tags, loaded_model_18, word2index, index2tag, max_len)\n",
    "result_df_model_18.to_csv(\"result_df_model_18.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_19 = result_df_model_previous(test_sentences, test_tags, loaded_model_19, word2index, index2tag, max_len)\n",
    "result_df_model_19.to_csv(\"result_df_model_19.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_20 = result_df_model_previous(test_sentences, test_tags, loaded_model_20, word2index, index2tag, max_len)\n",
    "result_df_model_20.to_csv(\"result_df_model_20.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_21 = result_df_model_previous(test_sentences, test_tags, loaded_model_21, word2index, index2tag, max_len)\n",
    "result_df_model_21.to_csv(\"result_df_model_21.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_22 = result_df_model_previous(test_sentences, test_tags, loaded_model_22, word2index, index2tag, max_len)\n",
    "result_df_model_22.to_csv(\"result_df_model_22.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_23 = result_df_model_previous(test_sentences, test_tags, loaded_model_23, word2index, index2tag, max_len)\n",
    "result_df_model_23.to_csv(\"result_df_model_23.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_24 = result_df_model_previous(test_sentences, test_tags, loaded_model_24, word2index, index2tag, max_len)\n",
    "result_df_model_24.to_csv(\"result_df_model_24.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_25 = result_df_model_previous(test_sentences, test_tags, loaded_model_25, word2index, index2tag, max_len)\n",
    "result_df_model_25.to_csv(\"result_df_model_25.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_26 = result_df_model_previous(test_sentences, test_tags, loaded_model_26, word2index, index2tag, max_len)\n",
    "result_df_model_26.to_csv(\"result_df_model_26.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = []\n",
    "results_by_model = []\n",
    "results_by_model_by_tag = []\n",
    "\n",
    "for filename in filenames:\n",
    "    f = os.path.join(current_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        print(f)\n",
    "\n",
    "    # Transformar os dados\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "\n",
    "    # Converter as colunas para listas\n",
    "    sentences = data.groupby(\"report\").apply(to_tuples).tolist()\n",
    "    texts, tags, tags_pred = tuple_2_list(sentences)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(filename)\n",
    "    result_dict = classification_report(tags, tags_pred, mode=\"strict\", scheme=IOB2, zero_division=False)\n",
    "    print(result_dict)\n",
    "    print()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
