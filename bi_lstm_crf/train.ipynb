{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m182.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /home/tlf/anaconda3/envs/bilstmcrf_env/lib/python3.8/site-packages (from seqeval) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/tlf/anaconda3/envs/bilstmcrf_env/lib/python3.8/site-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/tlf/anaconda3/envs/bilstmcrf_env/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/tlf/anaconda3/envs/bilstmcrf_env/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/tlf/anaconda3/envs/bilstmcrf_env/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=fcf9e0b9f2fbd640b75ec6a9d71458e1d6a4749a237f93df96eaa34baacc602a\n",
      "  Stored in directory: /home/tlf/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install seqeval\n",
    "#!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.scheme import IOB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 20:50:04.557560: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-16 20:50:04.751629: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-16 20:50:04.753505: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-16 20:50:05.550140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/tlf/anaconda3/envs/bilstmcrf_env/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.lookup import KeyValueTensorInitializer, StaticHashTable\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, TimeDistributed, Dropout, Bidirectional, Dense, Layer, InputSpec\n",
    "from tensorflow_addons.text import crf_log_likelihood, viterbi_decode, crf_decode\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "#import nervaluate\n",
    "#from nervaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.10.0\n",
      "Tensorflow Addons Version 0.21.0\n",
      "Numpy Version 1.25.2\n",
      "Pandas Version 1.5.3\n",
      "Keras Version 2.10.0\n",
      "Sklearn Version 1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version\", tf. __version__)\n",
    "print(\"Tensorflow Addons Version\", tfa. __version__)\n",
    "print(\"Numpy Version\", np.__version__)\n",
    "print(\"Pandas Version\", pd.__version__)\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"Sklearn Version\", sklearn.__version__)\n",
    "#print(\"Nervaluate Version\", nervaluate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.13.1\n",
      "Tensorflow Addons Version 0.21.0\n",
      "Numpy Version 1.24.3\n",
      "Pandas Version 2.0.3\n",
      "Keras Version 2.13.1\n",
      "Sklearn Version 1.3.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version\", tf. __version__)\n",
    "print(\"Tensorflow Addons Version\", tfa. __version__)\n",
    "print(\"Numpy Version\", np.__version__)\n",
    "print(\"Pandas Version\", pd.__version__)\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"Sklearn Version\", sklearn.__version__)\n",
    "#print(\"Nervaluate Version\", nervaluate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('laudos_1_963_iob.csv', encoding= 'utf-8', index_col=0)\n",
    "\n",
    "data = pd.read_csv('/home/tlf/Documents/mestrado/ner_models/data/df_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)\n",
    "data_train = pd.read_csv('/home/tlf/Documents/mestrado/ner_models/data/df_train_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"token\"].values.tolist(),\n",
    "                  data[\"iob_label\"].values.tolist())\n",
    "    return [(token, iob_label) for token, iob_label in iterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "  all_words = list(set(data[\"token\"].values))\n",
    "  all_tags = list(set(data[\"iob_label\"].values))\n",
    "\n",
    "  word2index = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
    "\n",
    "  word2index[\"--UNKNOWN_WORD--\"] = 0\n",
    "\n",
    "  word2index[\"--PADDING--\"] = 1\n",
    "\n",
    "  index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "  tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "  tag2index[\"--PADDING--\"] = 0\n",
    "\n",
    "  index2tag = {idx: word for word, idx in tag2index.items()}\n",
    "\n",
    "  return word2index, index2word, tag2index, index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(reports, word2index, tag2index):\n",
    "  contents = []\n",
    "  labels = []\n",
    "  for report in reports:\n",
    "    content = []\n",
    "    label = []\n",
    "    for i in range(len(report)):\n",
    "      token, iob_tag = report[i]\n",
    "      word_idx = word2index.get(token, 0)\n",
    "      tag_idx = tag2index.get(iob_tag, 0)\n",
    "      content.append(word_idx)\n",
    "      label.append(tag_idx)\n",
    "\n",
    "    contents.append(content)\n",
    "    labels.append(label)\n",
    "\n",
    "  \"\"\"\n",
    "  padding the array with max_sentence_size\n",
    "  pad_sequences(sequences, maxlen=None, dtype=\"int32\", padding=\"pre\", truncating=\"pre\", value=0.0,):\n",
    "  the maxlen argument if provided, or the length of the longest sequence in the list.\n",
    "  \"\"\"\n",
    "\n",
    "  max_sentence_size = 512\n",
    "  contents = tf.keras.preprocessing.sequence.pad_sequences(contents, maxlen=max_sentence_size, padding='post', value=1)\n",
    "  labels = tf.keras.preprocessing.sequence.pad_sequences(labels, maxlen=max_sentence_size, padding='post')\n",
    "\n",
    "  #max_sentence = len(contents[0])\n",
    "  tag_size = len(tag2index)\n",
    "\n",
    "  #print(max_sentence)\n",
    "\n",
    "  labels_categorical = [tf.keras.utils.to_categorical(i, num_classes=tag_size) for i in labels]\n",
    "  labels_categorical = np.asarray(labels_categorical)\n",
    "\n",
    "  return contents, labels, labels_categorical, max_sentence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['iob_label'].isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, index2word, tag2index, index2tag = build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'I-TAM',\n",
       " 2: 'I-ACH',\n",
       " 3: 'B-TAM',\n",
       " 4: 'I-ATE',\n",
       " 5: 'I-CAL',\n",
       " 6: 'B-BOR',\n",
       " 7: 'B-LOC',\n",
       " 8: 'B-CAL',\n",
       " 9: 'B-ACH',\n",
       " 10: 'I-BOR',\n",
       " 11: 'O',\n",
       " 12: 'B-ATE',\n",
       " 13: 'I-LOC',\n",
       " 0: '--PADDING--'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reports = data.groupby(\"report\").apply(to_tuples).tolist()\n",
    "reports = data_train.groupby(\"report\").apply(to_tuples).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences, tag_sequences, tag_sequences_categorical, max_len = tokenize(reports, word2index, tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(text_sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(input_dim, output_dim, input_length, mask_zero):\n",
    "    return Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length, mask_zero = mask_zero)\n",
    "\n",
    "def bilstm_crf(maxlen, n_tags, lstm_units, embedding_dim, n_words, mask_zero, training = True):\n",
    "    \"\"\"\n",
    "    bilstm_crf - module to build BiLSTM-CRF model\n",
    "    Inputs:\n",
    "        - input_shape : tuple\n",
    "            Tensor shape of inputs, excluding batch size\n",
    "    Outputs:\n",
    "        - output : tensorflow.keras.outputs.output\n",
    "            BiLSTM-CRF output\n",
    "    \"\"\"\n",
    "    input = Input(shape = (maxlen,))\n",
    "    # Embedding layer\n",
    "    embeddings = embedding_layer(input_dim = n_words, output_dim = embedding_dim, input_length = maxlen, mask_zero = mask_zero)\n",
    "    output = embeddings(input)\n",
    "\n",
    "    # BiLSTM layer\n",
    "    output = Bidirectional(LSTM(units = lstm_units, return_sequences = True, recurrent_dropout = 0.1))(output)\n",
    "\n",
    "    # Dense layer\n",
    "    output = TimeDistributed(Dense(n_tags, activation = 'relu'))(output)\n",
    "\n",
    "    output = CRF(n_tags, name = 'crf_layer')(output)\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sparse_target=True,\n",
    "                 transitions=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_dim (int): the number of labels to tag each temporal input.\n",
    "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
    "        Input shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        Output shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.output_dim = int(output_dim)\n",
    "        self.sparse_target = sparse_target\n",
    "        self.input_spec = InputSpec(min_ndim=3)\n",
    "        self.supports_masking = False\n",
    "        self.sequence_lengths = None\n",
    "        self.transitions = transitions\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        f_shape = tf.TensorShape(input_shape)\n",
    "        input_spec = InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
    "\n",
    "        if f_shape[-1] is None:\n",
    "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        if f_shape[-1] != self.output_dim:\n",
    "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
    "                             ' shape. Use a linear layer if needed.')\n",
    "        self.input_spec = input_spec\n",
    "        self.transitions = self.add_weight(name='transitions',\n",
    "                                           shape=[self.output_dim, self.output_dim],\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Just pass the received mask from previous layer, to the next layer or\n",
    "        # manipulate it if this layer changes the shape of the input\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
    "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        if sequence_lengths is not None:\n",
    "            assert len(sequence_lengths.shape) == 2\n",
    "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
    "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
    "            assert seq_len_shape[1] == 1\n",
    "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
    "        else:\n",
    "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
    "                tf.shape(inputs)[1]\n",
    "            )\n",
    "\n",
    "        viterbi_sequence, _ = crf_decode(sequences,\n",
    "                                         self.transitions,\n",
    "                                         self.sequence_lengths)\n",
    "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
    "        return K.in_train_phase(sequences, output)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        def crf_loss(y_true, y_pred):\n",
    "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "            log_likelihood, self.transitions = crf_log_likelihood(\n",
    "                y_pred,\n",
    "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "                self.sequence_lengths,\n",
    "                transition_params=self.transitions,\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        return crf_loss\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        def viterbi_accuracy(y_true, y_pred):\n",
    "            # -1e10 to avoid zero at sum(mask)\n",
    "            mask = K.cast(\n",
    "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            if self.sparse_target:\n",
    "                y_true = K.argmax(y_true, 2)\n",
    "            y_pred = K.cast(y_pred, 'int32')\n",
    "            y_true = K.cast(y_true, 'int32')\n",
    "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "            return K.sum(corrects * mask) / K.sum(mask)\n",
    "        return viterbi_accuracy\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
    "        return input_shape[:2] + (self.output_dim,)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CRF, self).get_config()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "            'sparse_target': self.sparse_target,\n",
    "            'transitions': self.transitions.numpy()  # Convert the transitions to a NumPy array\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Since 'transitions' is a NumPy array, we need to convert it back to a tensor\n",
    "        transitions = tf.convert_to_tensor(config['transitions'])\n",
    "        # Create a new instance of CRF with the saved configuration\n",
    "        return cls(output_dim=config['output_dim'], sparse_target=config['sparse_target'], transitions=transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_conf_matrix(cm, tagid):\n",
    "    tag_name = index2tag[tagid]\n",
    "    print(\"Tag name: {}\".format(tag_name))\n",
    "    print(cm[tagid])\n",
    "    tn, fp, fn, tp = cm[tagid].ravel()\n",
    "    tag_acc = round( ((tp + tn) / (tn + fp + fn + tp)), 3)\n",
    "\n",
    "\n",
    "    print(\"Tag accuracy: {:.3f}\\n\".format(tag_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_word_test_sentences_and_tags(index2tag, index2word, X_test, y_test):\n",
    "\n",
    "    test_sentences= []\n",
    "    test_tags = []\n",
    "\n",
    "    # Recupera os laudos e tags no formato word2index/tag2index\n",
    "    for i in range(len(X_test)):\n",
    "        aux_tag = []\n",
    "\n",
    "        report = \"\"\n",
    "        sentence = X_test[i]\n",
    "        tags = y_test[i]\n",
    "\n",
    "        # Recupera o laudo\n",
    "        for j in range(len(sentence)):\n",
    "            # Recupera a palavra\n",
    "            word = sentence[j]\n",
    "            # Recupera a tag\n",
    "            tag = tags[j]\n",
    "            \"\"\"\n",
    "            print(word)\n",
    "            print(tag)\n",
    "            \"\"\"\n",
    "            # A tag é predita em one-hot-enconding\n",
    "            # int_tag é o inteiro que representa a tag\n",
    "            # no dicionário index2tag\n",
    "            int_tag = np.where(tag == int(1))\n",
    "            \"\"\"\n",
    "            print(int_tag)\n",
    "            \"\"\"\n",
    "            # Constrói o laudo ignorando as palavras \"padding\"\n",
    "            # Constrói o array de tags do laudo\n",
    "            if str(index2word[word]) != '--PADDING--':\n",
    "                report = report + \" \" + str(index2word[word])\n",
    "                aux_tag.append(index2tag[int(int_tag[0][0])])\n",
    "\n",
    "        \"\"\"\n",
    "        print(report)\n",
    "        print(aux_tag)\n",
    "        \"\"\"\n",
    "        test_sentences.append(report)\n",
    "        test_tags.append(aux_tag)\n",
    "\n",
    "    return test_sentences, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_df_model(train_sentences, train_tags):\n",
    "\n",
    "    train_df = pd.DataFrame(columns = ['report', 'word', 'tag'])\n",
    "\n",
    "    for i in range (len(train_sentences)):\n",
    "\n",
    "        # Gera os laudos no formato index2word com o tamanho max_sentence\n",
    "\n",
    "        #print(\"LAUDO \" + str(i) + \"____________________________________________________________________________________________\")\n",
    "\n",
    "        sentence = train_sentences[i]\n",
    "        tags = train_tags[i]\n",
    "        \n",
    "        sentence = sentence.split()\n",
    "        \n",
    "        #print(len(padded_sentence))\n",
    "        #print(len(tags))\n",
    "        #print(len(pred[0]))\n",
    "        #print(pred[0])\n",
    "\n",
    "        if i < 10:\n",
    "            retval = \"\"\n",
    "            for w, t in zip(sentence, tags):\n",
    "                retval = retval + \"{:25}: {:10}\".format(w, t) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_0' + str(i)), 'word': w, 'tag' : t}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                train_df = pd.concat([train_df, df_new_row])\n",
    "                #test_df = test_df.append({'sentence': ('sentence_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}, ignore_index = True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            retval = \"\"\n",
    "            for w, t in zip(sentence, tags):\n",
    "                retval = retval + \"{:25}: {:10}\".format(w, t) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_' + str(i)), 'word': w, 'tag' : t}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                train_df = pd.concat([train_df, df_new_row])\n",
    "                #test_df = test_df.append({'sentence': ('sentence_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}, ignore_index = True)\n",
    "\n",
    "        #print(retval)\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_df_model_previous(test_sentences, test_tags, model, word2index, index2tag, MAX_SENTENCE):\n",
    "\n",
    "    test_df = pd.DataFrame(columns = ['report', 'word', 'tag', 'tag_pred'])\n",
    "\n",
    "    for i in range (len(test_sentences)):\n",
    "\n",
    "        # Gera os laudos no formato index2word com o tamanho max_sentence\n",
    "\n",
    "        #print(\"LAUDO \" + str(i) + \"____________________________________________________________________________________________\")\n",
    "        sentence = test_sentences[i]\n",
    "        tags = test_tags[i]\n",
    "        \n",
    "        sentence = sentence.split()\n",
    "        padded_sentence = sentence + [word2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(sentence))\n",
    "        padded_sentence = [word2index.get(w, 0) for w in padded_sentence]\n",
    "\n",
    "        # Faz a predição das tags das palavras\n",
    "        pred = model.predict(np.array([padded_sentence]))\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        #print(len(padded_sentence))\n",
    "        #print(len(tags))\n",
    "        #print(len(pred[0]))\n",
    "        #print(pred[0])\n",
    "\n",
    "        if i < 10:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "                #test_df = test_df.append({'sentence': ('sentence_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}, ignore_index = True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "                #test_df = test_df.append({'sentence': ('sentence_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}, ignore_index = True)\n",
    "\n",
    "        #print(retval)\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = text_sequences\n",
    "y_train = tag_sequences_categorical\n",
    "train_sentences, train_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, text_sequences, tag_sequences_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Tomografia computadorizada do Tórax Exame realizado em tomógrafo MULTISLICE Técnica : Exame realizado com aquisição volumétrica orientados por radiografia digital e reconstruções multiplanares sem a administração de contraste endovenoso . Análise : Arcabouço ósseo torácico conservado salientando pequena atitude escoliótica dorsal . Estruturas de partes moles da parede torácica sem particularidades . Ausência de coleções líquidas livres no espaço pleural . Parênquima pulmonar com morfologia , vasculatura e transparência normal , sem evidenciar nódulos ou áreas de consolidação . Não caracterizamos dilatações ou espessamento das paredes brônquicas . Traquéia , brônquios - fontes e hilos pulmonares livres . Estruturas mediastinais anatômicas , não sendo observadas linfonodomegalias em meio aos seus planos gordurosos . Impressão radiológica : Ausência de opacidades pulmonares focais sugestivas de processo infeccioso parenquimatoso em atividade .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 946,  212,  494, 3575,  486, 1400,  931, 1690, 3189,  816, 2524,\n",
       "       2970,  445,  797, 2018,  701, 1459,  407,  735, 1589,  987,  894,\n",
       "       1321, 3151, 3213,  469, 1069,  205, 2538, 2018, 1745, 2108,  987,\n",
       "       2098, 1487,  372, 1642, 1854,  582, 1314, 1913, 3184,  669, 3213,\n",
       "        469, 2203,  428, 2018,  612, 2264, 1228, 2787, 3399, 3398, 2071,\n",
       "        992,  535, 3381, 2018, 1887,  788,  613,  987, 2704, 2944, 2008,\n",
       "       3213,  626, 2753, 2648,  557,  535, 3213, 3493,  987, 3500,  545,\n",
       "       1014,  841, 3276, 2504, 3455,  500, 2575, 1589,  987,  894, 1321,\n",
       "       3151, 3213,  469, 1069,  205, 2538, 2098, 1487,  372, 1642, 1854,\n",
       "        582, 1183, 1833, 2134, 1663, 3151,  987, 1556,  987, 2676, 1087,\n",
       "       1069, 2985, 3317, 2018,  783, 3462, 2950, 3232, 2988, 3462, 2785,\n",
       "        757,  234, 3398, 1844, 3232, 1132, 3520,   83,  987, 2927, 1069,\n",
       "        142, 2409, 1014, 3276, 1482, 1833,  511, 3328, 2072, 1913, 1719,\n",
       "        616, 1855, 2018, 3422, 2907,  914, 1995, 1352, 1765, 2018,  745,\n",
       "       2018,  553, 2587, 1548, 2970,  724, 1913,  801,  987, 2159, 3218,\n",
       "       1491, 2926, 1458, 1597, 2717, 3213,   12, 3570,  987, 3305, 1830,\n",
       "        634, 2711, 2876, 2102, 2419, 2738, 1697, 1558, 1995, 3247,  501,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_sequences_categorical[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_DROPOUT = 0.1\n",
    "MAX_EPOCHS = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE_EMBEDDING_LIST = [25, 100, 300]\n",
    "LSTM_UNITS_LIST = [25, 50, 100]\n",
    "BATCH_SIZE_LIST = [4, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(word2index)\n",
    "n_tags = len(tag2index)\n",
    "lstm_dropout = LSTM_DROPOUT\n",
    "epochs = MAX_EPOCHS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 512, 50)           10200     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 512, 14)           714       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100985 (394.47 KB)\n",
      "Trainable params: 100985 (394.47 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 20:56:39.911665: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_BOOL\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'crf_loss/cond/output/_841'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 15s 354ms/step - loss: 287.5270 - viterbi_accuracy: 0.9049 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 9s 350ms/step - loss: 47.3500 - viterbi_accuracy: 0.9827 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 10s 380ms/step - loss: 34.9800 - viterbi_accuracy: 0.9829 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 10s 383ms/step - loss: 24.2131 - viterbi_accuracy: 0.9833 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 11s 407ms/step - loss: 20.3508 - viterbi_accuracy: 0.9851 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 10s 383ms/step - loss: 16.6599 - viterbi_accuracy: 0.9862 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 10s 381ms/step - loss: 14.2457 - viterbi_accuracy: 0.9875 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 10s 385ms/step - loss: 12.3491 - viterbi_accuracy: 0.9889 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 10s 388ms/step - loss: 10.1764 - viterbi_accuracy: 0.9928 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 10s 386ms/step - loss: 8.7063 - viterbi_accuracy: 0.9948 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a62e87994c0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_00 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_00.summary()\n",
    "\n",
    "# compile model\n",
    "model_00.compile(optimizer = Adam(learning_rate = lr), loss = model_00.layers[-1].loss, metrics = model_00.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "#model_00.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)\n",
    "\n",
    "model_00.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 512, 50)           10200     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 512, 14)           714       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100985 (394.47 KB)\n",
      "Trainable params: 100985 (394.47 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 15s 381ms/step - loss: 386.1848 - viterbi_accuracy: 0.8103 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 10s 384ms/step - loss: 48.5442 - viterbi_accuracy: 0.9829 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 10s 386ms/step - loss: 29.9046 - viterbi_accuracy: 0.9828 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 11s 392ms/step - loss: 21.1497 - viterbi_accuracy: 0.9846 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 10s 388ms/step - loss: 16.5449 - viterbi_accuracy: 0.9875 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 13.2754 - viterbi_accuracy: 0.9899 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 10s 385ms/step - loss: 11.0543 - viterbi_accuracy: 0.9921 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 11s 392ms/step - loss: 9.6146 - viterbi_accuracy: 0.9933 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 10s 372ms/step - loss: 8.1869 - viterbi_accuracy: 0.9943 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 10s 376ms/step - loss: 6.8037 - viterbi_accuracy: 0.9957 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a62ac0a4730>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_01 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_01.summary()\n",
    "\n",
    "# compile model\n",
    "model_01.compile(optimizer = Adam(learning_rate = lr), loss = model_01.layers[-1].loss, metrics = model_01.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_01.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 512, 50)           10200     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 512, 14)           714       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100985 (394.47 KB)\n",
      "Trainable params: 100985 (394.47 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 15s 358ms/step - loss: 485.5325 - viterbi_accuracy: 0.8319 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 10s 366ms/step - loss: 39.4540 - viterbi_accuracy: 0.9815 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 10s 362ms/step - loss: 24.1582 - viterbi_accuracy: 0.9845 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 10s 365ms/step - loss: 17.3054 - viterbi_accuracy: 0.9881 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 10s 362ms/step - loss: 13.1916 - viterbi_accuracy: 0.9914 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 10s 365ms/step - loss: 10.5152 - viterbi_accuracy: 0.9934 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 10s 365ms/step - loss: 8.7481 - viterbi_accuracy: 0.9947 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 10s 362ms/step - loss: 7.4920 - viterbi_accuracy: 0.9956 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 6.6463 - viterbi_accuracy: 0.9961 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 5.8765 - viterbi_accuracy: 0.9967 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a62a299a6d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_02 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_02.summary()\n",
    "\n",
    "# compile model\n",
    "model_02.compile(optimizer = Adam(learning_rate = lr), loss = model_02.layers[-1].loss, metrics = model_02.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_02.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 512, 100)          30400     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 512, 14)           1414      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121885 (476.11 KB)\n",
      "Trainable params: 121885 (476.11 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 16s 406ms/step - loss: 307.9014 - viterbi_accuracy: 0.8411 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 11s 402ms/step - loss: 33.4957 - viterbi_accuracy: 0.9833 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 11s 405ms/step - loss: 23.7730 - viterbi_accuracy: 0.9851 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 11s 404ms/step - loss: 18.9307 - viterbi_accuracy: 0.9863 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 11s 402ms/step - loss: 15.5804 - viterbi_accuracy: 0.9888 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 11s 404ms/step - loss: 13.3365 - viterbi_accuracy: 0.9918 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 11s 408ms/step - loss: 12.1315 - viterbi_accuracy: 0.9933 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 11s 405ms/step - loss: 10.6434 - viterbi_accuracy: 0.9945 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 11s 410ms/step - loss: 9.7507 - viterbi_accuracy: 0.9949 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 11s 401ms/step - loss: 8.5022 - viterbi_accuracy: 0.9952 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a629f956250>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_03 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_03.summary()\n",
    "\n",
    "# compile model\n",
    "model_03.compile(optimizer = Adam(learning_rate = lr), loss = model_03.layers[-1].loss, metrics = model_03.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_03.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, 512, 100)          30400     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDi  (None, 512, 14)           1414      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121885 (476.11 KB)\n",
      "Trainable params: 121885 (476.11 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 16s 402ms/step - loss: 264.3065 - viterbi_accuracy: 0.8815 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 11s 405ms/step - loss: 43.9588 - viterbi_accuracy: 0.9825 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 11s 404ms/step - loss: 29.0861 - viterbi_accuracy: 0.9834 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 11s 406ms/step - loss: 23.7376 - viterbi_accuracy: 0.9848 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 11s 404ms/step - loss: 19.2767 - viterbi_accuracy: 0.9859 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 11s 403ms/step - loss: 15.8765 - viterbi_accuracy: 0.9876 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 11s 406ms/step - loss: 13.4618 - viterbi_accuracy: 0.9905 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 11s 407ms/step - loss: 11.5125 - viterbi_accuracy: 0.9935 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 11s 406ms/step - loss: 9.9133 - viterbi_accuracy: 0.9945 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 11s 403ms/step - loss: 7.8711 - viterbi_accuracy: 0.9955 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a628c330dc0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_04 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_04.summary()\n",
    "\n",
    "# compile model\n",
    "model_04.compile(optimizer = Adam(learning_rate = lr), loss = model_04.layers[-1].loss, metrics = model_04.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_04.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, 512, 100)          30400     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDi  (None, 512, 14)           1414      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121885 (476.11 KB)\n",
      "Trainable params: 121885 (476.11 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 15s 400ms/step - loss: 215.1369 - viterbi_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 11s 401ms/step - loss: 44.6844 - viterbi_accuracy: 0.9820 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 29.4290 - viterbi_accuracy: 0.9818 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 11s 402ms/step - loss: 22.5679 - viterbi_accuracy: 0.9837 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 12s 429ms/step - loss: 17.5231 - viterbi_accuracy: 0.9868 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 12s 437ms/step - loss: 14.5093 - viterbi_accuracy: 0.9900 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 12s 435ms/step - loss: 11.7947 - viterbi_accuracy: 0.9928 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 12s 441ms/step - loss: 9.8425 - viterbi_accuracy: 0.9942 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 12s 453ms/step - loss: 8.7395 - viterbi_accuracy: 0.9945 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 11s 414ms/step - loss: 7.6827 - viterbi_accuracy: 0.9948 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a628af9d8e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_05 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_05.summary()\n",
    "\n",
    "# compile model\n",
    "model_05.compile(optimizer = Adam(learning_rate = lr), loss = model_05.layers[-1].loss, metrics = model_05.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_05.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 512, 200)          100800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 512, 14)           2814      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193685 (756.58 KB)\n",
      "Trainable params: 193685 (756.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 28s 835ms/step - loss: 514.3099 - viterbi_accuracy: 0.7721 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 58.4536 - viterbi_accuracy: 0.9801 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 43.6597 - viterbi_accuracy: 0.9824 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 29s 1s/step - loss: 31.0558 - viterbi_accuracy: 0.9827 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 26.1066 - viterbi_accuracy: 0.9827 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 27s 1s/step - loss: 22.7937 - viterbi_accuracy: 0.9828 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 27s 1s/step - loss: 19.2372 - viterbi_accuracy: 0.9844 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 16.1814 - viterbi_accuracy: 0.9862 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 27s 1s/step - loss: 14.1749 - viterbi_accuracy: 0.9872 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 12.1680 - viterbi_accuracy: 0.9878 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a6281b73dc0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_06 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_06.summary()\n",
    "\n",
    "# compile model\n",
    "model_06.compile(optimizer = Adam(learning_rate = lr), loss = model_06.layers[-1].loss, metrics = model_06.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_06.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 512, 200)          100800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 512, 14)           2814      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193685 (756.58 KB)\n",
      "Trainable params: 193685 (756.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 34s 1s/step - loss: 233.6386 - viterbi_accuracy: 0.8988 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 27s 1s/step - loss: 41.1800 - viterbi_accuracy: 0.9825 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 26.3423 - viterbi_accuracy: 0.9842 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 27s 1s/step - loss: 20.1077 - viterbi_accuracy: 0.9866 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 16.0260 - viterbi_accuracy: 0.9894 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 27s 1s/step - loss: 13.8014 - viterbi_accuracy: 0.9915 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 11.6373 - viterbi_accuracy: 0.9934 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 28s 1s/step - loss: 10.4969 - viterbi_accuracy: 0.9938 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 8.8942 - viterbi_accuracy: 0.9948 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 27s 1s/step - loss: 8.4988 - viterbi_accuracy: 0.9948 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a627ca2cdf0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_07 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_07.summary()\n",
    "\n",
    "# compile model\n",
    "model_07.compile(optimizer = Adam(learning_rate = lr), loss = model_07.layers[-1].loss, metrics = model_07.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_07.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_8 (Embedding)     (None, 512, 25)           89875     \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 512, 200)          100800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 512, 14)           2814      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193685 (756.58 KB)\n",
      "Trainable params: 193685 (756.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 31s 939ms/step - loss: 317.2578 - viterbi_accuracy: 0.8156 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 26s 960ms/step - loss: 46.7670 - viterbi_accuracy: 0.9822 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 31.9578 - viterbi_accuracy: 0.9822 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 26s 976ms/step - loss: 24.8558 - viterbi_accuracy: 0.9827 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 26s 961ms/step - loss: 20.7395 - viterbi_accuracy: 0.9836 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 17.7232 - viterbi_accuracy: 0.9868 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 26s 954ms/step - loss: 15.3209 - viterbi_accuracy: 0.9885 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 27s 986ms/step - loss: 13.3279 - viterbi_accuracy: 0.9890 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 26s 969ms/step - loss: 12.8303 - viterbi_accuracy: 0.9885 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 26s 974ms/step - loss: 10.9726 - viterbi_accuracy: 0.9891 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a62775c5730>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_08 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_08.summary()\n",
    "\n",
    "# compile model\n",
    "model_08.compile(optimizer = Adam(learning_rate = lr), loss = model_08.layers[-1].loss, metrics = model_08.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_08.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_9 (Embedding)     (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirecti  (None, 512, 50)           25200     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDi  (None, 512, 14)           714       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385610 (1.47 MB)\n",
      "Trainable params: 385610 (1.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 20s 478ms/step - loss: 244.4178 - viterbi_accuracy: 0.8807 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 13s 475ms/step - loss: 26.1199 - viterbi_accuracy: 0.9842 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 14s 534ms/step - loss: 16.2199 - viterbi_accuracy: 0.9899 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 14s 507ms/step - loss: 11.9540 - viterbi_accuracy: 0.9921 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 14s 519ms/step - loss: 9.4059 - viterbi_accuracy: 0.9950 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 13s 488ms/step - loss: 7.4512 - viterbi_accuracy: 0.9962 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 13s 480ms/step - loss: 6.0312 - viterbi_accuracy: 0.9970 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 13s 476ms/step - loss: 5.1767 - viterbi_accuracy: 0.9973 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 13s 485ms/step - loss: 4.9430 - viterbi_accuracy: 0.9971 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 13s 485ms/step - loss: 4.6414 - viterbi_accuracy: 0.9973 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a6272087910>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_09 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_09.summary()\n",
    "\n",
    "# compile model\n",
    "model_09.compile(optimizer = Adam(learning_rate = lr), loss = model_09.layers[-1].loss, metrics = model_09.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_09.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_10 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirect  (None, 512, 50)           25200     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_10 (TimeD  (None, 512, 14)           714       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385610 (1.47 MB)\n",
      "Trainable params: 385610 (1.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 20s 509ms/step - loss: 564.0030 - viterbi_accuracy: 0.6781 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 13s 482ms/step - loss: 32.5764 - viterbi_accuracy: 0.9821 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 13s 465ms/step - loss: 20.8037 - viterbi_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 12s 458ms/step - loss: 16.0016 - viterbi_accuracy: 0.9890 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 15s 542ms/step - loss: 12.3101 - viterbi_accuracy: 0.9924 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 13s 472ms/step - loss: 8.7028 - viterbi_accuracy: 0.9947 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 14s 504ms/step - loss: 7.0406 - viterbi_accuracy: 0.9955 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 13s 474ms/step - loss: 6.0194 - viterbi_accuracy: 0.9960 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 13s 464ms/step - loss: 4.8242 - viterbi_accuracy: 0.9971 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 12s 460ms/step - loss: 4.0185 - viterbi_accuracy: 0.9978 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a626cfda6d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_10 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_10.summary()\n",
    "\n",
    "# compile model\n",
    "model_10.compile(optimizer = Adam(learning_rate = lr), loss = model_10.layers[-1].loss, metrics = model_10.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_10.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_11 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 512, 50)           25200     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 512, 14)           714       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385610 (1.47 MB)\n",
      "Trainable params: 385610 (1.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 20s 499ms/step - loss: 246.9259 - viterbi_accuracy: 0.9093 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 13s 487ms/step - loss: 27.5326 - viterbi_accuracy: 0.9835 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 13s 480ms/step - loss: 20.1525 - viterbi_accuracy: 0.9874 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 13s 487ms/step - loss: 15.6726 - viterbi_accuracy: 0.9903 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 13s 485ms/step - loss: 12.5818 - viterbi_accuracy: 0.9915 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 13s 472ms/step - loss: 10.4857 - viterbi_accuracy: 0.9932 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 13s 484ms/step - loss: 8.8712 - viterbi_accuracy: 0.9963 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 13s 493ms/step - loss: 7.5849 - viterbi_accuracy: 0.9971 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 13s 482ms/step - loss: 6.4147 - viterbi_accuracy: 0.9977 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 13s 478ms/step - loss: 5.5889 - viterbi_accuracy: 0.9978 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a62600c85e0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_11 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_11.summary()\n",
    "\n",
    "# compile model\n",
    "model_11.compile(optimizer = Adam(learning_rate = lr), loss = model_11.layers[-1].loss, metrics = model_11.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_11.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_12 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_12 (Bidirect  (None, 512, 100)          60400     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_12 (TimeD  (None, 512, 14)           1414      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 421510 (1.61 MB)\n",
      "Trainable params: 421510 (1.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 24s 642ms/step - loss: 203.4314 - viterbi_accuracy: 0.9267 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 17s 645ms/step - loss: 27.9032 - viterbi_accuracy: 0.9847 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 17s 640ms/step - loss: 14.5728 - viterbi_accuracy: 0.9905 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 17s 644ms/step - loss: 8.9729 - viterbi_accuracy: 0.9946 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 17s 640ms/step - loss: 6.4604 - viterbi_accuracy: 0.9963 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 17s 634ms/step - loss: 5.0424 - viterbi_accuracy: 0.9970 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 17s 613ms/step - loss: 4.0605 - viterbi_accuracy: 0.9976 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 17s 620ms/step - loss: 3.5049 - viterbi_accuracy: 0.9979 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 16s 609ms/step - loss: 3.0244 - viterbi_accuracy: 0.9982 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 17s 621ms/step - loss: 2.8939 - viterbi_accuracy: 0.9981 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a624ff80430>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_12 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_12.summary()\n",
    "\n",
    "# compile model\n",
    "model_12.compile(optimizer = Adam(learning_rate = lr), loss = model_12.layers[-1].loss, metrics = model_12.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_12.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_13 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_13 (Bidirect  (None, 512, 100)          60400     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_13 (TimeD  (None, 512, 14)           1414      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 421510 (1.61 MB)\n",
      "Trainable params: 421510 (1.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 22s 610ms/step - loss: 380.5576 - viterbi_accuracy: 0.8481 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 17s 621ms/step - loss: 22.1789 - viterbi_accuracy: 0.9858 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 17s 616ms/step - loss: 12.8981 - viterbi_accuracy: 0.9920 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 18s 676ms/step - loss: 7.9516 - viterbi_accuracy: 0.9957 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 17s 638ms/step - loss: 5.4755 - viterbi_accuracy: 0.9971 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 16s 612ms/step - loss: 4.4116 - viterbi_accuracy: 0.9976 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 18s 657ms/step - loss: 3.8764 - viterbi_accuracy: 0.9976 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 16s 607ms/step - loss: 3.0126 - viterbi_accuracy: 0.9982 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 16s 608ms/step - loss: 2.4774 - viterbi_accuracy: 0.9986 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 16s 605ms/step - loss: 2.3338 - viterbi_accuracy: 0.9987 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a624c11ef40>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_13 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_13.summary()\n",
    "\n",
    "# compile model\n",
    "model_13.compile(optimizer = Adam(learning_rate = lr), loss = model_13.layers[-1].loss, metrics = model_13.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_13.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_14 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirect  (None, 512, 100)          60400     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_14 (TimeD  (None, 512, 14)           1414      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 421510 (1.61 MB)\n",
      "Trainable params: 421510 (1.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 23s 642ms/step - loss: 207.2569 - viterbi_accuracy: 0.9160 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 17s 645ms/step - loss: 37.9522 - viterbi_accuracy: 0.9826 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 17s 635ms/step - loss: 24.0582 - viterbi_accuracy: 0.9846 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 17s 635ms/step - loss: 18.0827 - viterbi_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 17s 645ms/step - loss: 14.1673 - viterbi_accuracy: 0.9894 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 18s 684ms/step - loss: 11.1553 - viterbi_accuracy: 0.9917 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 17s 642ms/step - loss: 8.5946 - viterbi_accuracy: 0.9936 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 18s 653ms/step - loss: 7.3739 - viterbi_accuracy: 0.9954 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 18s 676ms/step - loss: 6.2306 - viterbi_accuracy: 0.9964 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 18s 652ms/step - loss: 5.5937 - viterbi_accuracy: 0.9968 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a6237ec2ee0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_14 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_14.summary()\n",
    "\n",
    "# compile model\n",
    "model_14.compile(optimizer = Adam(learning_rate = lr), loss = model_14.layers[-1].loss, metrics = model_14.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_14.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_15 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_15 (Bidirect  (None, 512, 200)          160800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_15 (TimeD  (None, 512, 14)           2814      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 523310 (2.00 MB)\n",
      "Trainable params: 523310 (2.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 42s 1s/step - loss: 198.3271 - viterbi_accuracy: 0.9143 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 35s 1s/step - loss: 31.8176 - viterbi_accuracy: 0.9839 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 35s 1s/step - loss: 22.8073 - viterbi_accuracy: 0.9855 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 38s 1s/step - loss: 17.5141 - viterbi_accuracy: 0.9897 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 13.5484 - viterbi_accuracy: 0.9935 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 33s 1s/step - loss: 11.8008 - viterbi_accuracy: 0.9942 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 9.6673 - viterbi_accuracy: 0.9951 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 33s 1s/step - loss: 8.2077 - viterbi_accuracy: 0.9954 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 35s 1s/step - loss: 7.2191 - viterbi_accuracy: 0.9957 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 34s 1s/step - loss: 5.9328 - viterbi_accuracy: 0.9966 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a622b4b2730>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_15 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_15.summary()\n",
    "\n",
    "# compile model\n",
    "model_15.compile(optimizer = Adam(learning_rate = lr), loss = model_15.layers[-1].loss, metrics = model_15.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_15.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_16 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_16 (Bidirect  (None, 512, 200)          160800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_16 (TimeD  (None, 512, 14)           2814      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 523310 (2.00 MB)\n",
      "Trainable params: 523310 (2.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 43s 1s/step - loss: 191.0865 - viterbi_accuracy: 0.9116 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 36s 1s/step - loss: 32.9777 - viterbi_accuracy: 0.9828 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 36s 1s/step - loss: 19.0269 - viterbi_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 13.0903 - viterbi_accuracy: 0.9903 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 36s 1s/step - loss: 10.0849 - viterbi_accuracy: 0.9924 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 31s 1s/step - loss: 8.7169 - viterbi_accuracy: 0.9942 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 31s 1s/step - loss: 7.1209 - viterbi_accuracy: 0.9953 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 5.8220 - viterbi_accuracy: 0.9959 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 5.1626 - viterbi_accuracy: 0.9969 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 31s 1s/step - loss: 4.5111 - viterbi_accuracy: 0.9972 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a621f304490>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_16 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_16.summary()\n",
    "\n",
    "# compile model\n",
    "model_16.compile(optimizer = Adam(learning_rate = lr), loss = model_16.layers[-1].loss, metrics = model_16.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_16.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_17 (Embedding)    (None, 512, 100)          359500    \n",
      "                                                                 \n",
      " bidirectional_17 (Bidirect  (None, 512, 200)          160800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_17 (TimeD  (None, 512, 14)           2814      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 523310 (2.00 MB)\n",
      "Trainable params: 523310 (2.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 37s 1s/step - loss: 208.8552 - viterbi_accuracy: 0.9086 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 33s 1s/step - loss: 38.1936 - viterbi_accuracy: 0.9826 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 24.2360 - viterbi_accuracy: 0.9851 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 33s 1s/step - loss: 18.5655 - viterbi_accuracy: 0.9874 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 16.1962 - viterbi_accuracy: 0.9907 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 13.2732 - viterbi_accuracy: 0.9939 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 32s 1s/step - loss: 11.2347 - viterbi_accuracy: 0.9949 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 25s 930ms/step - loss: 10.0797 - viterbi_accuracy: 0.9954 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 23s 840ms/step - loss: 8.0335 - viterbi_accuracy: 0.9957 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 25s 945ms/step - loss: 5.9416 - viterbi_accuracy: 0.9965 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a621e92adc0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_17 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_17.summary()\n",
    "\n",
    "# compile model\n",
    "model_17.compile(optimizer = Adam(learning_rate = lr), loss = model_17.layers[-1].loss, metrics = model_17.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_17.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_18 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_18 (Bidirect  (None, 512, 50)           65200     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_18 (TimeD  (None, 512, 14)           714       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1144610 (4.37 MB)\n",
      "Trainable params: 1144610 (4.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 21s 549ms/step - loss: 181.3146 - viterbi_accuracy: 0.9278 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 13s 466ms/step - loss: 28.7483 - viterbi_accuracy: 0.9837 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 12s 451ms/step - loss: 19.6795 - viterbi_accuracy: 0.9879 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 12s 447ms/step - loss: 12.6313 - viterbi_accuracy: 0.9925 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 12s 446ms/step - loss: 8.9862 - viterbi_accuracy: 0.9947 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 12s 458ms/step - loss: 6.9673 - viterbi_accuracy: 0.9957 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 12s 461ms/step - loss: 5.7039 - viterbi_accuracy: 0.9967 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 12s 454ms/step - loss: 5.0896 - viterbi_accuracy: 0.9968 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 12s 448ms/step - loss: 4.4901 - viterbi_accuracy: 0.9971 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 12s 451ms/step - loss: 3.9115 - viterbi_accuracy: 0.9975 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a621d6109d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_18 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_18.summary()\n",
    "\n",
    "# compile model\n",
    "model_18.compile(optimizer = Adam(learning_rate = lr), loss = model_18.layers[-1].loss, metrics = model_18.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_18.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_19 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_19 (Bidirect  (None, 512, 50)           65200     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_19 (TimeD  (None, 512, 14)           714       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1144610 (4.37 MB)\n",
      "Trainable params: 1144610 (4.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 17s 451ms/step - loss: 211.8211 - viterbi_accuracy: 0.9123 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 12s 455ms/step - loss: 25.5528 - viterbi_accuracy: 0.9857 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 12s 448ms/step - loss: 19.0403 - viterbi_accuracy: 0.9887 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 12s 445ms/step - loss: 15.1539 - viterbi_accuracy: 0.9902 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 12s 447ms/step - loss: 12.3884 - viterbi_accuracy: 0.9922 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 12s 452ms/step - loss: 8.6752 - viterbi_accuracy: 0.9939 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 12s 450ms/step - loss: 7.0480 - viterbi_accuracy: 0.9952 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 5.4452 - viterbi_accuracy: 0.9966 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 12s 452ms/step - loss: 4.6317 - viterbi_accuracy: 0.9974 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 12s 449ms/step - loss: 4.0112 - viterbi_accuracy: 0.9978 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a621d31fbb0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_19 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_19.summary()\n",
    "\n",
    "# compile model\n",
    "model_19.compile(optimizer = Adam(learning_rate = lr), loss = model_19.layers[-1].loss, metrics = model_19.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_19.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_20 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_20 (Bidirect  (None, 512, 50)           65200     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_20 (TimeD  (None, 512, 14)           714       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1144610 (4.37 MB)\n",
      "Trainable params: 1144610 (4.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 18s 478ms/step - loss: 715.1423 - viterbi_accuracy: 0.5954 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 13s 482ms/step - loss: 28.4588 - viterbi_accuracy: 0.9834 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 15s 568ms/step - loss: 17.2195 - viterbi_accuracy: 0.9889 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 15s 548ms/step - loss: 11.8488 - viterbi_accuracy: 0.9925 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 14s 516ms/step - loss: 9.0840 - viterbi_accuracy: 0.9947 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 14s 526ms/step - loss: 7.3409 - viterbi_accuracy: 0.9956 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 14s 525ms/step - loss: 6.2792 - viterbi_accuracy: 0.9963 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 14s 519ms/step - loss: 5.4529 - viterbi_accuracy: 0.9969 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 14s 526ms/step - loss: 4.9348 - viterbi_accuracy: 0.9972 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 14s 513ms/step - loss: 4.2921 - viterbi_accuracy: 0.9977 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a621333fe50>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_20 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_20.summary()\n",
    "\n",
    "# compile model\n",
    "model_20.compile(optimizer = Adam(learning_rate = lr), loss = model_20.layers[-1].loss, metrics = model_20.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_20.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_21 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_21 (Bidirect  (None, 512, 100)          140400    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_21 (TimeD  (None, 512, 14)           1414      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1220510 (4.66 MB)\n",
      "Trainable params: 1220510 (4.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 28s 803ms/step - loss: 842.3880 - viterbi_accuracy: 0.8623 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 22s 803ms/step - loss: 609.1670 - viterbi_accuracy: 0.9822 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 22s 807ms/step - loss: 71.8314 - viterbi_accuracy: 0.9822 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 22s 807ms/step - loss: 19.0574 - viterbi_accuracy: 0.9860 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 22s 802ms/step - loss: 13.9153 - viterbi_accuracy: 0.9891 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 22s 808ms/step - loss: 11.9044 - viterbi_accuracy: 0.9901 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 22s 802ms/step - loss: 10.6686 - viterbi_accuracy: 0.9935 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 22s 798ms/step - loss: 9.6513 - viterbi_accuracy: 0.9946 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 22s 804ms/step - loss: 8.5753 - viterbi_accuracy: 0.9952 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 22s 816ms/step - loss: 7.6689 - viterbi_accuracy: 0.9954 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a6211ecdbe0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_21 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_21.summary()\n",
    "\n",
    "# compile model\n",
    "model_21.compile(optimizer = Adam(learning_rate = lr), loss = model_21.layers[-1].loss, metrics = model_21.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_21.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_22 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_22 (Bidirect  (None, 512, 100)          140400    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_22 (TimeD  (None, 512, 14)           1414      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1220510 (4.66 MB)\n",
      "Trainable params: 1220510 (4.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 27s 810ms/step - loss: 134.6273 - viterbi_accuracy: 0.9415 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 22s 798ms/step - loss: 25.4470 - viterbi_accuracy: 0.9855 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 22s 804ms/step - loss: 13.6532 - viterbi_accuracy: 0.9914 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 22s 803ms/step - loss: 9.6343 - viterbi_accuracy: 0.9932 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 22s 800ms/step - loss: 7.6789 - viterbi_accuracy: 0.9945 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 22s 814ms/step - loss: 6.4329 - viterbi_accuracy: 0.9955 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 22s 801ms/step - loss: 5.6006 - viterbi_accuracy: 0.9962 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 21s 796ms/step - loss: 5.0815 - viterbi_accuracy: 0.9962 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 22s 813ms/step - loss: 4.6122 - viterbi_accuracy: 0.9971 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 22s 800ms/step - loss: 3.9467 - viterbi_accuracy: 0.9977 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a62112e5460>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_22 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_22.summary()\n",
    "\n",
    "# compile model\n",
    "model_22.compile(optimizer = Adam(learning_rate = lr), loss = model_22.layers[-1].loss, metrics = model_22.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_22.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_23 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_23 (Bidirect  (None, 512, 100)          140400    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_23 (TimeD  (None, 512, 14)           1414      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1220510 (4.66 MB)\n",
      "Trainable params: 1220510 (4.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 27s 827ms/step - loss: 170.2527 - viterbi_accuracy: 0.9323 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 22s 810ms/step - loss: 18.1209 - viterbi_accuracy: 0.9880 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 23s 853ms/step - loss: 11.2787 - viterbi_accuracy: 0.9925 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 22s 794ms/step - loss: 8.1354 - viterbi_accuracy: 0.9952 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 22s 809ms/step - loss: 6.3531 - viterbi_accuracy: 0.9963 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 22s 810ms/step - loss: 5.1908 - viterbi_accuracy: 0.9974 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 21s 793ms/step - loss: 4.4004 - viterbi_accuracy: 0.9979 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 22s 801ms/step - loss: 4.0325 - viterbi_accuracy: 0.9982 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 22s 797ms/step - loss: 3.4199 - viterbi_accuracy: 0.9985 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 21s 792ms/step - loss: 2.9582 - viterbi_accuracy: 0.9986 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a6210b38ee0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_23 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_23.summary()\n",
    "\n",
    "# compile model\n",
    "model_23.compile(optimizer = Adam(learning_rate = lr), loss = model_23.layers[-1].loss, metrics = model_23.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_23.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_24 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_24 (Bidirect  (None, 512, 200)          320800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_24 (TimeD  (None, 512, 14)           2814      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1402310 (5.35 MB)\n",
      "Trainable params: 1402310 (5.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 49s 2s/step - loss: 203.3939 - viterbi_accuracy: 0.8820 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 43s 2s/step - loss: 26.1491 - viterbi_accuracy: 0.9852 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 43s 2s/step - loss: 17.8853 - viterbi_accuracy: 0.9894 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 12.2703 - viterbi_accuracy: 0.9919 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 9.8503 - viterbi_accuracy: 0.9929 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 8.6634 - viterbi_accuracy: 0.9939 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 8.1172 - viterbi_accuracy: 0.9941 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 43s 2s/step - loss: 7.1999 - viterbi_accuracy: 0.9955 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 40s 1s/step - loss: 6.2384 - viterbi_accuracy: 0.9961 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 5.6037 - viterbi_accuracy: 0.9970 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a620331af70>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_24 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_24.summary()\n",
    "\n",
    "# compile model\n",
    "model_24.compile(optimizer = Adam(learning_rate = lr), loss = model_24.layers[-1].loss, metrics = model_24.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_24.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_25 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_25 (Bidirect  (None, 512, 200)          320800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_25 (TimeD  (None, 512, 14)           2814      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1402310 (5.35 MB)\n",
      "Trainable params: 1402310 (5.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 48s 2s/step - loss: 155.3490 - viterbi_accuracy: 0.9181 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 29.7886 - viterbi_accuracy: 0.9833 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 17.6673 - viterbi_accuracy: 0.9876 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 13.3408 - viterbi_accuracy: 0.9904 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 10.9858 - viterbi_accuracy: 0.9919 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 9.5916 - viterbi_accuracy: 0.9933 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 8.2789 - viterbi_accuracy: 0.9947 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 6.8512 - viterbi_accuracy: 0.9960 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 6.0587 - viterbi_accuracy: 0.9969 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 5.8399 - viterbi_accuracy: 0.9968 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a61f7ebb850>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_25 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_25.summary()\n",
    "\n",
    "# compile model\n",
    "model_25.compile(optimizer = Adam(learning_rate = lr), loss = model_25.layers[-1].loss, metrics = model_25.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_25.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_27 (InputLayer)       [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_26 (Embedding)    (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_26 (Bidirect  (None, 512, 200)          320800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_26 (TimeD  (None, 512, 14)           2814      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1402310 (5.35 MB)\n",
      "Trainable params: 1402310 (5.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 50s 2s/step - loss: 149.2998 - viterbi_accuracy: 0.9294 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 35.0159 - viterbi_accuracy: 0.9828 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 25.4709 - viterbi_accuracy: 0.9832 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 21.6631 - viterbi_accuracy: 0.9849 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 18.3953 - viterbi_accuracy: 0.9869 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 15.7471 - viterbi_accuracy: 0.9883 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 11.9345 - viterbi_accuracy: 0.9910 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 9.6478 - viterbi_accuracy: 0.9937 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 43s 2s/step - loss: 8.4581 - viterbi_accuracy: 0.9940 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 44s 2s/step - loss: 7.4461 - viterbi_accuracy: 0.9943 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a61eab887c0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model_26 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_26.summary()\n",
    "\n",
    "# compile model\n",
    "model_26.compile(optimizer = Adam(learning_rate = lr), loss = model_26.layers[-1].loss, metrics = model_26.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_26.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('/home/tlf/Documents/mestrado/ner_models/data/df_test_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)\n",
    "reports = data_test.groupby(\"report\").apply(to_tuples).tolist()\n",
    "test_text_sequences, test_tag_sequences, test_tag_sequences_categorical, max_len = tokenize(reports, word2index, tag2index)\n",
    "\n",
    "X_test = test_text_sequences\n",
    "y_test = test_tag_sequences_categorical\n",
    "\n",
    "test_sentences, test_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, test_text_sequences, test_tag_sequences_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 458ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 489ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 451ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 1s 622ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 1s 607ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 1s 546ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 1s 638ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 1s 553ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 1s 606ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 1s 526ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 1s 608ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 1s 870ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 1s 652ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 1s 675ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 1s 630ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n"
     ]
    }
   ],
   "source": [
    "result_df_model_00 = result_df_model_previous(test_sentences, test_tags, model_00, word2index, index2tag, max_len)\n",
    "result_df_model_00.to_csv(\"result_df_model_00.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_01 = result_df_model_previous(test_sentences, test_tags, model_01, word2index, index2tag, max_len)\n",
    "result_df_model_01.to_csv(\"result_df_model_01.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_02 = result_df_model_previous(test_sentences, test_tags, model_02, word2index, index2tag, max_len)\n",
    "result_df_model_02.to_csv(\"result_df_model_02.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_03 = result_df_model_previous(test_sentences, test_tags, model_03, word2index, index2tag, max_len)\n",
    "result_df_model_03.to_csv(\"result_df_model_03.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_04 = result_df_model_previous(test_sentences, test_tags, model_04, word2index, index2tag, max_len)\n",
    "result_df_model_04.to_csv(\"result_df_model_04.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_05 = result_df_model_previous(test_sentences, test_tags, model_05, word2index, index2tag, max_len)\n",
    "result_df_model_05.to_csv(\"result_df_model_05.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_06 = result_df_model_previous(test_sentences, test_tags, model_06, word2index, index2tag, max_len)\n",
    "result_df_model_06.to_csv(\"result_df_model_06.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_07 = result_df_model_previous(test_sentences, test_tags, model_07, word2index, index2tag, max_len)\n",
    "result_df_model_07.to_csv(\"result_df_model_07.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_08 = result_df_model_previous(test_sentences, test_tags, model_08, word2index, index2tag, max_len)\n",
    "result_df_model_08.to_csv(\"result_df_model_08.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_09 = result_df_model_previous(test_sentences, test_tags, model_09, word2index, index2tag, max_len)\n",
    "result_df_model_09.to_csv(\"result_df_model_09.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_10 = result_df_model_previous(test_sentences, test_tags, model_10, word2index, index2tag, max_len)\n",
    "result_df_model_10.to_csv(\"result_df_model_10.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_11 = result_df_model_previous(test_sentences, test_tags, model_11, word2index, index2tag, max_len)\n",
    "result_df_model_11.to_csv(\"result_df_model_11.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_12 = result_df_model_previous(test_sentences, test_tags, model_12, word2index, index2tag, max_len)\n",
    "result_df_model_12.to_csv(\"result_df_model_12.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_13 = result_df_model_previous(test_sentences, test_tags, model_13, word2index, index2tag, max_len)\n",
    "result_df_model_13.to_csv(\"result_df_model_13.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_14 = result_df_model_previous(test_sentences, test_tags, model_14, word2index, index2tag, max_len)\n",
    "result_df_model_14.to_csv(\"result_df_model_14.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_15 = result_df_model_previous(test_sentences, test_tags, model_15, word2index, index2tag, max_len)\n",
    "result_df_model_15.to_csv(\"result_df_model_15.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_16 = result_df_model_previous(test_sentences, test_tags, model_16, word2index, index2tag, max_len)\n",
    "result_df_model_16.to_csv(\"result_df_model_16.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_17 = result_df_model_previous(test_sentences, test_tags, model_17, word2index, index2tag, max_len)\n",
    "result_df_model_17.to_csv(\"result_df_model_17.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_18 = result_df_model_previous(test_sentences, test_tags, model_18, word2index, index2tag, max_len)\n",
    "result_df_model_18.to_csv(\"result_df_model_18.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_19 = result_df_model_previous(test_sentences, test_tags, model_19, word2index, index2tag, max_len)\n",
    "result_df_model_19.to_csv(\"result_df_model_19.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_20 = result_df_model_previous(test_sentences, test_tags, model_20, word2index, index2tag, max_len)\n",
    "result_df_model_20.to_csv(\"result_df_model_20.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_21 = result_df_model_previous(test_sentences, test_tags, model_21, word2index, index2tag, max_len)\n",
    "result_df_model_21.to_csv(\"result_df_model_21.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_22 = result_df_model_previous(test_sentences, test_tags, model_22, word2index, index2tag, max_len)\n",
    "result_df_model_22.to_csv(\"result_df_model_22.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_23 = result_df_model_previous(test_sentences, test_tags, model_23, word2index, index2tag, max_len)\n",
    "result_df_model_23.to_csv(\"result_df_model_23.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_24 = result_df_model_previous(test_sentences, test_tags, model_24, word2index, index2tag, max_len)\n",
    "result_df_model_24.to_csv(\"result_df_model_24.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_25 = result_df_model_previous(test_sentences, test_tags, model_25, word2index, index2tag, max_len)\n",
    "result_df_model_25.to_csv(\"result_df_model_25.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_26 = result_df_model_previous(test_sentences, test_tags, model_26, word2index, index2tag, max_len)\n",
    "result_df_model_26.to_csv(\"result_df_model_26.csv\", encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_00/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_00/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_01/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_01/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_02/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_02/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_03/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_03/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_04/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_04/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_06/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_06/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_07/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_07/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_08/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_08/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_09/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_09/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_11/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_11/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_12/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_13/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_13/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_14/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_14/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_15/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_15/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_16/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_16/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_17/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_17/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_19/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_19/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_20/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_20/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_21/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_21/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_22/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_22/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_23/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_23/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_24/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_24/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_25/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_25/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_26/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_26/assets\n"
     ]
    }
   ],
   "source": [
    "model_00.save(\"model_00\")\n",
    "model_01.save(\"model_01\")\n",
    "model_02.save(\"model_02\")\n",
    "model_03.save(\"model_03\")\n",
    "model_04.save(\"model_04\")\n",
    "model_05.save(\"model_05\")\n",
    "model_06.save(\"model_06\")\n",
    "model_07.save(\"model_07\")\n",
    "model_08.save(\"model_08\")\n",
    "model_09.save(\"model_09\")\n",
    "model_10.save(\"model_10\")\n",
    "model_11.save(\"model_11\")\n",
    "model_12.save(\"model_12\")\n",
    "model_13.save(\"model_13\")\n",
    "model_14.save(\"model_14\")\n",
    "model_15.save(\"model_15\")\n",
    "model_16.save(\"model_16\")\n",
    "model_17.save(\"model_17\")\n",
    "model_18.save(\"model_18\")\n",
    "model_19.save(\"model_19\")\n",
    "model_20.save(\"model_20\")\n",
    "model_21.save(\"model_21\")\n",
    "model_22.save(\"model_22\")\n",
    "model_23.save(\"model_23\")\n",
    "model_24.save(\"model_24\")\n",
    "model_25.save(\"model_25\")\n",
    "model_26.save(\"model_26\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models - TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_00.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:30:56 tensorflow INFO: Assets written to: model_00.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_01.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:31:16 tensorflow INFO: Assets written to: model_01.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_02.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:31:35 tensorflow INFO: Assets written to: model_02.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_03.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:31:49 tensorflow INFO: Assets written to: model_03.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_04.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:32:07 tensorflow INFO: Assets written to: model_04.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_05.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:32:26 tensorflow INFO: Assets written to: model_05.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_06.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:32:44 tensorflow INFO: Assets written to: model_06.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_07.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:33:02 tensorflow INFO: Assets written to: model_07.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_08.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:33:21 tensorflow INFO: Assets written to: model_08.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_09.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:33:40 tensorflow INFO: Assets written to: model_09.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_10.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:33:59 tensorflow INFO: Assets written to: model_10.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_11.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:34:18 tensorflow INFO: Assets written to: model_11.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_12.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:34:32 tensorflow INFO: Assets written to: model_12.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_13.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:34:50 tensorflow INFO: Assets written to: model_13.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_14.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:35:09 tensorflow INFO: Assets written to: model_14.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_15.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:35:28 tensorflow INFO: Assets written to: model_15.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_16.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:35:46 tensorflow INFO: Assets written to: model_16.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_17.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:36:05 tensorflow INFO: Assets written to: model_17.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_18.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:36:24 tensorflow INFO: Assets written to: model_18.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_19.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:36:43 tensorflow INFO: Assets written to: model_19.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_20.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:36:57 tensorflow INFO: Assets written to: model_20.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_21.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:37:16 tensorflow INFO: Assets written to: model_21.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_22.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:37:34 tensorflow INFO: Assets written to: model_22.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_23.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:37:52 tensorflow INFO: Assets written to: model_23.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_24.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:38:11 tensorflow INFO: Assets written to: model_24.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_25.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:38:29 tensorflow INFO: Assets written to: model_25.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_26.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:38:49 tensorflow INFO: Assets written to: model_26.tf\\assets\n"
     ]
    }
   ],
   "source": [
    "\"\"\"model_00.save(\"model_00.tf\")\n",
    "model_01.save(\"model_01.tf\")\n",
    "model_02.save(\"model_02.tf\")\n",
    "model_03.save(\"model_03.tf\")\n",
    "model_04.save(\"model_04.tf\")\n",
    "model_05.save(\"model_05.tf\")\n",
    "model_06.save(\"model_06.tf\")\n",
    "model_07.save(\"model_07.tf\")\n",
    "model_08.save(\"model_08.tf\")\n",
    "model_09.save(\"model_09.tf\")\n",
    "model_10.save(\"model_10.tf\")\n",
    "model_11.save(\"model_11.tf\")\n",
    "model_12.save(\"model_12.tf\")\n",
    "model_13.save(\"model_13.tf\")\n",
    "model_14.save(\"model_14.tf\")\n",
    "model_15.save(\"model_15.tf\")\n",
    "model_16.save(\"model_16.tf\")\n",
    "model_17.save(\"model_17.tf\")\n",
    "model_18.save(\"model_18.tf\")\n",
    "model_19.save(\"model_19.tf\")\n",
    "model_20.save(\"model_20.tf\")\n",
    "model_21.save(\"model_21.tf\")\n",
    "model_22.save(\"model_22.tf\")\n",
    "model_23.save(\"model_23.tf\")\n",
    "model_24.save(\"model_24.tf\")\n",
    "model_25.save(\"model_25.tf\")\n",
    "model_26.save(\"model_26.tf\")\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models - H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_00.save(\"model_00.h5\")\n",
    "model_01.save(\"model_01.h5\")\n",
    "model_02.save(\"model_02.h5\")\n",
    "model_03.save(\"model_03.h5\")\n",
    "model_04.save(\"model_04.h5\")\n",
    "model_05.save(\"model_05.h5\")\n",
    "model_06.save(\"model_06.h5\")\n",
    "model_07.save(\"model_07.h5\")\n",
    "model_08.save(\"model_08.h5\")\n",
    "model_09.save(\"model_09.h5\")\n",
    "model_10.save(\"model_10.h5\")\n",
    "model_11.save(\"model_11.h5\")\n",
    "model_12.save(\"model_12.h5\")\n",
    "model_13.save(\"model_13.h5\")\n",
    "model_14.save(\"model_14.h5\")\n",
    "model_15.save(\"model_15.h5\")\n",
    "model_16.save(\"model_16.h5\")\n",
    "model_17.save(\"model_17.h5\")\n",
    "model_18.save(\"model_18.h5\")\n",
    "model_19.save(\"model_19.h5\")\n",
    "model_20.save(\"model_20.h5\")\n",
    "model_21.save(\"model_21.h5\")\n",
    "model_22.save(\"model_22.h5\")\n",
    "model_23.save(\"model_23.h5\")\n",
    "model_24.save(\"model_24.h5\")\n",
    "model_25.save(\"model_25.h5\")\n",
    "model_26.save(\"model_26.h5\")\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"word\"].values.tolist(),\n",
    "                  data[\"tag\"].values.tolist(),\n",
    "                  data[\"tag_pred\"].values.tolist())\n",
    "    return [(word, tag, tag_pred) for word, tag, tag_pred in iterator]\n",
    "\n",
    "def tuple_2_list(sentences):\n",
    "    texts = [[word[0] for word in sentence] for sentence in sentences]\n",
    "    tags = [[word[1] for word in sentence] for sentence in sentences]\n",
    "    tags_pred = [[word[2] for word in sentence] for sentence in sentences]\n",
    "\n",
    "    return texts, tags, tags_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = ['O',\n",
    "'ACH',\n",
    "'ATE',\n",
    "'BORDAS',\n",
    "'CAL',\n",
    "'LOC',\n",
    "'TAM',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_directory = 'c:\\\\Users\\\\tarci\\\\Desktop\\\\modelos_mestrado\\\\bilstm_antigo\\\\'\n",
    "current_directory = os.getcwd()+\"/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(current_directory)\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['result_df_model_00.csv', 'result_df_model_01.csv', 'result_df_model_02.csv', 'result_df_model_03.csv', 'result_df_model_04.csv', 'result_df_model_05.csv', 'result_df_model_06.csv', 'result_df_model_07.csv', 'result_df_model_08.csv', 'result_df_model_09.csv', 'result_df_model_10.csv', 'result_df_model_11.csv', 'result_df_model_12.csv', 'result_df_model_13.csv', 'result_df_model_14.csv', 'result_df_model_15.csv', 'result_df_model_16.csv', 'result_df_model_17.csv', 'result_df_model_18.csv', 'result_df_model_19.csv', 'result_df_model_20.csv', 'result_df_model_21.csv', 'result_df_model_22.csv', 'result_df_model_23.csv', 'result_df_model_24.csv', 'result_df_model_25.csv', 'result_df_model_26.csv']\n"
     ]
    }
   ],
   "source": [
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_00.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 28\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mtag = data.tag.tolist()\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mtag_pred = data.tag_pred.tolist()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mprint(tag_pred)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Avaliação\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluator\u001b[49m(tags, tags_pred, tags\u001b[38;5;241m=\u001b[39mall_tags, loader\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Returns overall metrics and metrics for each tag\u001b[39;00m\n\u001b[1;32m     31\u001b[0m results, results_by_tag \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = []\n",
    "results_by_model = []\n",
    "results_by_model_by_tag = []\n",
    "\n",
    "for filename in filenames:\n",
    "    f = os.path.join(current_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        print(f)\n",
    "\n",
    "    # Transformar os dados\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "\n",
    "    # Converter as colunas para listas\n",
    "    sentences = data.groupby(\"report\").apply(to_tuples).tolist()\n",
    "    texts, tags, tags_pred = tuple_2_list(sentences)\n",
    "    \n",
    "    \"\"\"\n",
    "    tag = data.tag.tolist()\n",
    "    tag_pred = data.tag_pred.tolist()\n",
    "    print(classification_report(tag, tag_pred, target_names=all_tags))\n",
    "    print(text)\n",
    "    print(tag)\n",
    "    print(tag_pred)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Avaliação\n",
    "    evaluator = Evaluator(tags, tags_pred, tags=all_tags, loader=\"list\")\n",
    "\n",
    "    # Returns overall metrics and metrics for each tag\n",
    "    results, results_by_tag = evaluator.evaluate()\n",
    "    \n",
    "    \"\"\"\n",
    "    print(results)\n",
    "    print(results_by_tag)\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name.append(filename)\n",
    "    results_by_model.append(results)\n",
    "    results_by_model_by_tag.append(results_by_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_model_20_df = pd.DataFrame(results_by_model[20])\n",
    "\n",
    "metrics_localizacao_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Localização\"])\n",
    "\n",
    "metrics_calcificacao_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Calcificação\"])\n",
    "\n",
    "metrics_achado_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Achado\"])\n",
    "\n",
    "metrics_bordas_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Bordas\"])\n",
    "\n",
    "metrics_atenuacao_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Atenuação\"])\n",
    "\n",
    "metrics_tamanho_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Tamanho\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_model_20_df.to_csv(\"metrics_model_20.csv\")\n",
    "metrics_localizacao_model_20_df.to_csv(\"metrics_localizacao_model_20.csv\")\n",
    "metrics_calcificacao_model_20_df.to_csv(\"metrics_calcificacao_model_20.csv\")\n",
    "metrics_achado_model_20_df.to_csv(\"metrics_achado_model_20.csv\")\n",
    "metrics_bordas_model_20_df.to_csv(\"metrics_bordas_model_20.csv\")\n",
    "metrics_atenuacao_model_20_df.to_csv(\"metrics_atenuacao_model_20.csv\")\n",
    "metrics_tamanho_model_20_df.to_csv(\"metrics_tamanho_model_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': {'ent_type': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}, 'partial': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}, 'strict': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}, 'exact': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}}, 'Achado': {'ent_type': {'correct': 120, 'incorrect': 0, 'partial': 0, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9523809523809523, 'recall': 0.8888888888888888, 'f1': 0.9195402298850575}, 'partial': {'correct': 116, 'incorrect': 0, 'partial': 4, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9365079365079365, 'recall': 0.8740740740740741, 'f1': 0.9042145593869733}, 'strict': {'correct': 116, 'incorrect': 4, 'partial': 0, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9206349206349206, 'recall': 0.8592592592592593, 'f1': 0.8888888888888888}, 'exact': {'correct': 116, 'incorrect': 4, 'partial': 0, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9206349206349206, 'recall': 0.8592592592592593, 'f1': 0.8888888888888888}}, 'Atenuação': {'ent_type': {'correct': 20, 'incorrect': 1, 'partial': 0, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.8, 'recall': 0.8, 'f1': 0.8000000000000002}, 'partial': {'correct': 18, 'incorrect': 0, 'partial': 3, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.78, 'recall': 0.78, 'f1': 0.78}, 'strict': {'correct': 18, 'incorrect': 3, 'partial': 0, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.72, 'recall': 0.72, 'f1': 0.72}, 'exact': {'correct': 18, 'incorrect': 3, 'partial': 0, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.72, 'recall': 0.72, 'f1': 0.72}}, 'Bordas': {'ent_type': {'correct': 6, 'incorrect': 1, 'partial': 0, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1': 0.6666666666666666}, 'partial': {'correct': 5, 'incorrect': 0, 'partial': 2, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1': 0.6666666666666666}, 'strict': {'correct': 5, 'incorrect': 2, 'partial': 0, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.5555555555555556, 'recall': 0.5555555555555556, 'f1': 0.5555555555555556}, 'exact': {'correct': 5, 'incorrect': 2, 'partial': 0, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.5555555555555556, 'recall': 0.5555555555555556, 'f1': 0.5555555555555556}}, 'Calcificação': {'ent_type': {'correct': 71, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9342105263157895, 'recall': 1.0, 'f1': 0.9659863945578232}, 'partial': {'correct': 69, 'incorrect': 0, 'partial': 2, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9210526315789473, 'recall': 0.9859154929577465, 'f1': 0.9523809523809524}, 'strict': {'correct': 69, 'incorrect': 2, 'partial': 0, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9078947368421053, 'recall': 0.971830985915493, 'f1': 0.9387755102040817}, 'exact': {'correct': 69, 'incorrect': 2, 'partial': 0, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9078947368421053, 'recall': 0.971830985915493, 'f1': 0.9387755102040817}}, 'Localização': {'ent_type': {'correct': 125, 'incorrect': 1, 'partial': 0, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8992805755395683, 'recall': 0.8741258741258742, 'f1': 0.8865248226950354}, 'partial': {'correct': 114, 'incorrect': 0, 'partial': 12, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8633093525179856, 'recall': 0.8391608391608392, 'f1': 0.851063829787234}, 'strict': {'correct': 113, 'incorrect': 13, 'partial': 0, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8129496402877698, 'recall': 0.7902097902097902, 'f1': 0.8014184397163121}, 'exact': {'correct': 114, 'incorrect': 12, 'partial': 0, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8201438848920863, 'recall': 0.7972027972027972, 'f1': 0.8085106382978723}}, 'Tamanho': {'ent_type': {'correct': 114, 'incorrect': 2, 'partial': 0, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8636363636363636, 'recall': 0.926829268292683, 'f1': 0.8941176470588236}, 'partial': {'correct': 114, 'incorrect': 0, 'partial': 2, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8712121212121212, 'recall': 0.9349593495934959, 'f1': 0.9019607843137255}, 'strict': {'correct': 112, 'incorrect': 4, 'partial': 0, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8484848484848485, 'recall': 0.9105691056910569, 'f1': 0.8784313725490196}, 'exact': {'correct': 114, 'incorrect': 2, 'partial': 0, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8636363636363636, 'recall': 0.926829268292683, 'f1': 0.8941176470588236}}}\n"
     ]
    }
   ],
   "source": [
    "best_model_tags_ent_type = []\n",
    "best_model_tags_partial = []\n",
    "best_model_tags_strict = []\n",
    "best_model_tags_exact = []\n",
    "\n",
    "best_model = results_by_model_by_tag[20]\n",
    "\n",
    "print(best_model)\n",
    "\n",
    "for entity in all_tags:\n",
    "    tag_ent_type = []\n",
    "    tag_partial = []\n",
    "    tag_strict = []\n",
    "    tag_exact = []\n",
    "\n",
    "    entity = str(entity)\n",
    "    # entity_type\n",
    "    ent_precision = round(best_model[entity][\"ent_type\"][\"precision\"], 2)\n",
    "    ent_recall = round(best_model[entity][\"ent_type\"][\"recall\"], 2)\n",
    "    ent_f1 = round(best_model[entity][\"ent_type\"][\"f1\"], 2)\n",
    "    tag_ent_type.append(ent_precision)\n",
    "    tag_ent_type.append(ent_recall)\n",
    "    tag_ent_type.append(ent_f1)\n",
    "\n",
    "    # partial\n",
    "    partial_precision = round(best_model[entity][\"partial\"][\"precision\"], 2)\n",
    "    partial_recall = round(best_model[entity][\"partial\"][\"recall\"], 2)\n",
    "    partial_f1 = round(best_model[entity][\"partial\"][\"f1\"], 2)\n",
    "    tag_partial.append(partial_precision)\n",
    "    tag_partial.append(partial_recall)\n",
    "    tag_partial.append(partial_f1)\n",
    "\n",
    "    # strict\n",
    "    strict_precision = round(best_model[entity][\"strict\"][\"precision\"], 2)\n",
    "    strict_recall = round(best_model[entity][\"strict\"][\"recall\"], 2)\n",
    "    strict_f1 = round(best_model[entity][\"strict\"][\"f1\"], 2)\n",
    "    tag_strict.append(strict_precision)\n",
    "    tag_strict.append(strict_recall)\n",
    "    tag_strict.append(strict_f1)\n",
    "\n",
    "    # exact\n",
    "    exact_precision = round(best_model[entity][\"exact\"][\"precision\"], 2)\n",
    "    exact_recall = round(best_model[entity][\"exact\"][\"recall\"], 2)\n",
    "    exact_f1 = round(best_model[entity][\"exact\"][\"f1\"], 2)\n",
    "    tag_exact.append(exact_precision)\n",
    "    tag_exact.append(exact_recall)\n",
    "    tag_exact.append(exact_f1)\n",
    "\n",
    "    best_model_tags_ent_type.append(tag_ent_type)\n",
    "    best_model_tags_partial.append(tag_partial)\n",
    "    best_model_tags_strict.append(tag_strict)\n",
    "    best_model_tags_exact.append(tag_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_tags_ent_type_np = np.array(best_model_tags_ent_type)\n",
    "best_model_tags_partial_np = np.array(best_model_tags_partial)\n",
    "best_model_tags_strict_np = np.array(best_model_tags_strict)\n",
    "best_model_tags_exact_np = np.array(best_model_tags_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_best_model_df = pd.DataFrame(best_model_tags_ent_type_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "partial_results_best_model_df = pd.DataFrame(best_model_tags_partial_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "strict_results_best_model_df = pd.DataFrame(best_model_tags_strict_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "exact_results_best_model_df = pd.DataFrame(best_model_tags_exact_np, columns = [\"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_best_model_df.to_csv(\"ent_type_results_best_model_df.csv\", encoding=\"utf-8\")\n",
    "partial_results_best_model_df.to_csv(\"partial_results_best_model_df.csv\", encoding=\"utf-8\")\n",
    "strict_results_best_model_df.to_csv(\"strict_results_best_model_df.csv\", encoding=\"utf-8\")\n",
    "exact_results_best_model_df.to_csv(\"exact_results_best_model_df.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar os Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ent_type = []\n",
    "models_partial = []\n",
    "models_strict = []\n",
    "models_exact = []\n",
    "\n",
    "for result_model in results_by_model:\n",
    "    ent_type = []\n",
    "    partial = []\n",
    "    strict = []\n",
    "    exact = []\n",
    "\n",
    "    # entity_type\n",
    "    ent_precision = round(result_model[\"ent_type\"][\"precision\"], 2)\n",
    "    ent_recall = round(result_model[\"ent_type\"][\"recall\"], 2)\n",
    "    ent_f1 = round(result_model[\"ent_type\"][\"f1\"], 2)\n",
    "    ent_type.append(ent_precision)\n",
    "    ent_type.append(ent_recall)\n",
    "    ent_type.append(ent_f1)\n",
    "\n",
    "    # partial\n",
    "    partial_precision = round(result_model[\"partial\"][\"precision\"], 2)\n",
    "    partial_recall = round(result_model[\"partial\"][\"recall\"], 2)\n",
    "    partial_f1 = round(result_model[\"partial\"][\"f1\"], 2)\n",
    "    partial.append(partial_precision)\n",
    "    partial.append(partial_recall)\n",
    "    partial.append(partial_f1)\n",
    "\n",
    "    # strict\n",
    "    strict_precision = round(result_model[\"strict\"][\"precision\"], 2)\n",
    "    strict_recall = round(result_model[\"strict\"][\"recall\"], 2)\n",
    "    strict_f1 = round(result_model[\"strict\"][\"f1\"], 2)\n",
    "    strict.append(strict_precision)\n",
    "    strict.append(strict_recall)\n",
    "    strict.append(strict_f1)\n",
    "\n",
    "    # exact\n",
    "    exact_precision = round(result_model[\"exact\"][\"precision\"], 2)\n",
    "    exact_recall = round(result_model[\"exact\"][\"recall\"], 2)\n",
    "    exact_f1 = round(result_model[\"exact\"][\"f1\"], 2)\n",
    "    exact.append(exact_precision)\n",
    "    exact.append(exact_recall)\n",
    "    exact.append(exact_f1)\n",
    "\n",
    "\n",
    "    models_ent_type.append(ent_type)\n",
    "    models_partial.append(partial)\n",
    "    models_strict.append(strict)\n",
    "    models_exact.append(exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ent_type_np = np.array(models_ent_type)\n",
    "models_partial_np = np.array(models_partial)\n",
    "models_strict_np = np.array(models_strict)\n",
    "models_exact_np = np.array(models_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_by_model_df = pd.DataFrame(models_ent_type_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "partial_results_by_model_df = pd.DataFrame(models_partial_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "strict_results_by_model_df = pd.DataFrame(models_strict_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "exact_results_by_model_df = pd.DataFrame(models_exact_np, columns = [\"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_by_model_df.to_csv(\"ent_type_results_by_model_df.csv\", encoding=\"utf-8\")\n",
    "partial_results_by_model_df.to_csv(\"partial_results_by_model_df.csv\", encoding=\"utf-8\")\n",
    "strict_results_by_model_df.to_csv(\"strict_results_by_model_df.csv\", encoding=\"utf-8\")\n",
    "exact_results_by_model_df.to_csv(\"exact_results_by_model_df.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = 'c:\\\\Users\\\\tarci\\\\Desktop\\\\modelos_mestrado\\\\bilstm_antigo\\\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 25s 588ms/step - loss: 312.4252 - viterbi_accuracy: 0.9078 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x200d04ef100>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test =  bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "\n",
    "# compile model\n",
    "model_test.compile(optimizer = Adam(learning_rate = lr), loss = model_test.layers[-1].loss, metrics = model_test.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "model_test.fit(X_train, y_train, epochs = 1, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_teste_load\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 16:00:01 tensorflow INFO: Assets written to: model_teste_load\\assets\n"
     ]
    }
   ],
   "source": [
    "model_test.save(\"model_teste_load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessário passar essas funções "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_accuracy(y_true, y_pred):\n",
    "    mask = K.cast(K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "    shape = tf.shape(y_pred)\n",
    "    sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "    y_pred, _ = crf_decode(y_pred, K.zeros_like(y_pred), sequence_lengths)\n",
    "    if K.ndim(y_true) == K.ndim(y_pred) - 1:\n",
    "        y_true = K.expand_dims(y_true, K.ndim(y_pred) - 1)\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "    return K.sum(corrects * mask) / K.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_loss(y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=CRF(dtype='float32').dtype)\n",
    "    log_likelihood, _ = tf.keras.layers.CRF(dtype='float32')(y_pred, y_true)\n",
    "    return tf.reduce_mean(-log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('model_teste_load', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 24s 593ms/step - loss: 61.3024 - viterbi_accuracy: 0.9674 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x200c7aad300>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "loaded_model.compile(optimizer = Adam(learning_rate = lr), loss = loaded_model.layers[-1].loss, metrics = loaded_model.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "loaded_model.fit(X_train, y_train, epochs = 1, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_00.csv\n",
      "\n",
      "\n",
      "result_df_model_00.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.68      0.70      0.69        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.64      0.68      0.66        57\n",
      "         LOC       0.73      0.71      0.72        73\n",
      "         TAM       0.70      0.82      0.76        62\n",
      "\n",
      "   micro avg       0.69      0.68      0.68       287\n",
      "   macro avg       0.46      0.49      0.47       287\n",
      "weighted avg       0.64      0.68      0.66       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_01.csv\n",
      "\n",
      "\n",
      "result_df_model_01.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.74      0.84      0.78        74\n",
      "         ATE       0.44      0.25      0.32        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.63      0.70      0.67        57\n",
      "         LOC       0.79      0.79      0.79        73\n",
      "         TAM       0.67      0.39      0.49        62\n",
      "\n",
      "   micro avg       0.71      0.66      0.68       287\n",
      "   macro avg       0.55      0.50      0.51       287\n",
      "weighted avg       0.69      0.66      0.66       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_02.csv\n",
      "\n",
      "\n",
      "result_df_model_02.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.74      0.81      0.77        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.62      0.72      0.67        57\n",
      "         LOC       0.88      0.82      0.85        73\n",
      "         TAM       0.85      0.73      0.78        62\n",
      "\n",
      "   micro avg       0.77      0.72      0.74       287\n",
      "   macro avg       0.52      0.51      0.51       287\n",
      "weighted avg       0.72      0.72      0.72       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_03.csv\n",
      "\n",
      "\n",
      "result_df_model_03.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.70      0.31      0.43        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.71      0.26      0.38        57\n",
      "         LOC       0.86      0.77      0.81        73\n",
      "         TAM       0.53      0.87      0.66        62\n",
      "\n",
      "   micro avg       0.67      0.52      0.58       287\n",
      "   macro avg       0.47      0.37      0.38       287\n",
      "weighted avg       0.66      0.52      0.54       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_04.csv\n",
      "\n",
      "\n",
      "result_df_model_04.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.88      0.49      0.63        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.56      0.70      0.62        57\n",
      "         LOC       0.60      0.38      0.47        73\n",
      "         TAM       0.84      0.77      0.81        62\n",
      "\n",
      "   micro avg       0.70      0.53      0.60       287\n",
      "   macro avg       0.48      0.39      0.42       287\n",
      "weighted avg       0.67      0.53      0.58       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_05.csv\n",
      "\n",
      "\n",
      "result_df_model_05.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.55      0.57      0.56        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.00      0.00      0.00        57\n",
      "         LOC       0.20      0.18      0.19        73\n",
      "         TAM       0.90      0.69      0.78        62\n",
      "\n",
      "   micro avg       0.52      0.34      0.41       287\n",
      "   macro avg       0.28      0.24      0.26       287\n",
      "weighted avg       0.39      0.34      0.36       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_06.csv\n",
      "\n",
      "\n",
      "result_df_model_06.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.68      0.62      0.65        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.83      0.09      0.16        57\n",
      "         LOC       0.00      0.00      0.00        73\n",
      "         TAM       0.50      0.02      0.03        62\n",
      "\n",
      "   micro avg       0.68      0.18      0.29       287\n",
      "   macro avg       0.33      0.12      0.14       287\n",
      "weighted avg       0.45      0.18      0.21       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_07.csv\n",
      "\n",
      "\n",
      "result_df_model_07.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.73      0.43      0.54        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.70      0.46      0.55        57\n",
      "         LOC       0.85      0.85      0.85        73\n",
      "         TAM       0.45      0.77      0.57        62\n",
      "\n",
      "   micro avg       0.65      0.59      0.61       287\n",
      "   macro avg       0.46      0.42      0.42       287\n",
      "weighted avg       0.64      0.59      0.59       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_08.csv\n",
      "\n",
      "\n",
      "result_df_model_08.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.77      0.14      0.23        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.43      0.05      0.09        57\n",
      "         LOC       0.12      0.22      0.16        73\n",
      "         TAM       0.00      0.00      0.00        62\n",
      "\n",
      "   micro avg       0.19      0.10      0.13       287\n",
      "   macro avg       0.22      0.07      0.08       287\n",
      "weighted avg       0.31      0.10      0.12       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_09.csv\n",
      "\n",
      "\n",
      "result_df_model_09.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.77      0.80      0.78        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.58      0.70      0.63        57\n",
      "         LOC       0.87      0.84      0.85        73\n",
      "         TAM       0.86      0.87      0.86        62\n",
      "\n",
      "   micro avg       0.77      0.75      0.76       287\n",
      "   macro avg       0.51      0.53      0.52       287\n",
      "weighted avg       0.72      0.75      0.73       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_10.csv\n",
      "\n",
      "\n",
      "result_df_model_10.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.86      0.82      0.84        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.77      0.82      0.80        57\n",
      "         LOC       0.88      0.89      0.88        73\n",
      "         TAM       0.73      0.79      0.76        62\n",
      "\n",
      "   micro avg       0.81      0.77      0.79       287\n",
      "   macro avg       0.54      0.55      0.55       287\n",
      "weighted avg       0.76      0.77      0.76       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_11.csv\n",
      "\n",
      "\n",
      "result_df_model_11.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.87      0.53      0.66        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.63      0.68      0.66        57\n",
      "         LOC       0.87      0.85      0.86        73\n",
      "         TAM       0.66      0.81      0.72        62\n",
      "\n",
      "   micro avg       0.75      0.66      0.70       287\n",
      "   macro avg       0.50      0.48      0.48       287\n",
      "weighted avg       0.71      0.66      0.67       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_12.csv\n",
      "\n",
      "\n",
      "result_df_model_12.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.86      0.95      0.90        74\n",
      "         ATE       0.25      0.06      0.10        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.96      0.91      0.94        57\n",
      "         LOC       0.90      0.84      0.87        73\n",
      "         TAM       0.87      0.87      0.87        62\n",
      "\n",
      "   micro avg       0.88      0.83      0.86       287\n",
      "   macro avg       0.64      0.60      0.61       287\n",
      "weighted avg       0.84      0.83      0.83       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_13.csv\n",
      "\n",
      "\n",
      "result_df_model_13.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.92      0.93      0.93        74\n",
      "         ATE       0.75      0.94      0.83        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.95      0.93      0.94        57\n",
      "         LOC       0.93      0.90      0.92        73\n",
      "         TAM       0.89      0.94      0.91        62\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       287\n",
      "   macro avg       0.74      0.77      0.75       287\n",
      "weighted avg       0.90      0.91      0.90       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_14.csv\n",
      "\n",
      "\n",
      "result_df_model_14.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.73      0.69      0.71        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.62      0.49      0.55        57\n",
      "         LOC       0.84      0.71      0.77        73\n",
      "         TAM       0.57      0.76      0.65        62\n",
      "\n",
      "   micro avg       0.69      0.62      0.65       287\n",
      "   macro avg       0.46      0.44      0.45       287\n",
      "weighted avg       0.65      0.62      0.63       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_15.csv\n",
      "\n",
      "\n",
      "result_df_model_15.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.88      0.80      0.84        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.76      0.72      0.74        57\n",
      "         LOC       0.69      0.74      0.72        73\n",
      "         TAM       0.90      0.85      0.88        62\n",
      "\n",
      "   micro avg       0.80      0.72      0.76       287\n",
      "   macro avg       0.54      0.52      0.53       287\n",
      "weighted avg       0.75      0.72      0.73       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_16.csv\n",
      "\n",
      "\n",
      "result_df_model_16.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.67      0.81      0.73        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.57      0.60      0.58        57\n",
      "         LOC       0.74      0.77      0.75        73\n",
      "         TAM       0.72      0.92      0.81        62\n",
      "\n",
      "   micro avg       0.68      0.72      0.70       287\n",
      "   macro avg       0.45      0.52      0.48       287\n",
      "weighted avg       0.63      0.72      0.67       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_17.csv\n",
      "\n",
      "\n",
      "result_df_model_17.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.76      0.46      0.57        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.76      0.67      0.71        57\n",
      "         LOC       0.35      0.49      0.41        73\n",
      "         TAM       0.79      0.55      0.65        62\n",
      "\n",
      "   micro avg       0.59      0.49      0.54       287\n",
      "   macro avg       0.44      0.36      0.39       287\n",
      "weighted avg       0.60      0.49      0.53       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_18.csv\n",
      "\n",
      "\n",
      "result_df_model_18.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.85      0.85      0.85        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.64      0.72      0.68        57\n",
      "         LOC       0.92      0.89      0.90        73\n",
      "         TAM       0.71      0.92      0.80        62\n",
      "\n",
      "   micro avg       0.78      0.79      0.78       287\n",
      "   macro avg       0.52      0.56      0.54       287\n",
      "weighted avg       0.73      0.79      0.76       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_19.csv\n",
      "\n",
      "\n",
      "result_df_model_19.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.73      0.81      0.77        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.64      0.63      0.64        57\n",
      "         LOC       0.87      0.82      0.85        73\n",
      "         TAM       0.75      0.82      0.78        62\n",
      "\n",
      "   micro avg       0.75      0.72      0.74       287\n",
      "   macro avg       0.50      0.51      0.51       287\n",
      "weighted avg       0.70      0.72      0.71       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_20.csv\n",
      "\n",
      "\n",
      "result_df_model_20.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.75      0.80      0.77        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.58      0.72      0.64        57\n",
      "         LOC       0.89      0.89      0.89        73\n",
      "         TAM       0.87      0.94      0.90        62\n",
      "\n",
      "   micro avg       0.77      0.78      0.77       287\n",
      "   macro avg       0.51      0.56      0.53       287\n",
      "weighted avg       0.72      0.78      0.75       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_21.csv\n",
      "\n",
      "\n",
      "result_df_model_21.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       1.00      0.28      0.44        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.62      0.68      0.65        57\n",
      "         LOC       0.90      0.74      0.81        73\n",
      "         TAM       0.00      0.00      0.00        62\n",
      "\n",
      "   micro avg       0.79      0.40      0.53       287\n",
      "   macro avg       0.42      0.28      0.32       287\n",
      "weighted avg       0.61      0.40      0.45       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_22.csv\n",
      "\n",
      "\n",
      "result_df_model_22.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.78      0.81      0.79        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.65      0.63      0.64        57\n",
      "         LOC       0.73      0.74      0.73        73\n",
      "         TAM       0.72      0.87      0.79        62\n",
      "\n",
      "   micro avg       0.73      0.71      0.72       287\n",
      "   macro avg       0.48      0.51      0.49       287\n",
      "weighted avg       0.67      0.71      0.69       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_23.csv\n",
      "\n",
      "\n",
      "result_df_model_23.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.67      0.80      0.73        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.93      0.91      0.92        57\n",
      "         LOC       0.91      0.86      0.89        73\n",
      "         TAM       0.90      0.90      0.90        62\n",
      "\n",
      "   micro avg       0.84      0.80      0.82       287\n",
      "   macro avg       0.57      0.58      0.57       287\n",
      "weighted avg       0.78      0.80      0.79       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_24.csv\n",
      "\n",
      "\n",
      "result_df_model_24.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.51      0.66      0.57        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.62      0.51      0.56        57\n",
      "         LOC       0.75      0.73      0.74        73\n",
      "         TAM       0.91      0.95      0.93        62\n",
      "\n",
      "   micro avg       0.68      0.66      0.67       287\n",
      "   macro avg       0.46      0.47      0.47       287\n",
      "weighted avg       0.64      0.66      0.65       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_25.csv\n",
      "\n",
      "\n",
      "result_df_model_25.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.52      0.49      0.50        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.37      0.51      0.43        57\n",
      "         LOC       0.44      0.52      0.48        73\n",
      "         TAM       0.71      0.56      0.63        62\n",
      "\n",
      "   micro avg       0.49      0.48      0.49       287\n",
      "   macro avg       0.34      0.35      0.34       287\n",
      "weighted avg       0.48      0.48      0.47       287\n",
      "\n",
      "\n",
      "\n",
      "/home/tlf/Documents/mestrado/ner_models/bi_lstm_crf/results/result_df_model_26.csv\n",
      "\n",
      "\n",
      "result_df_model_26.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACH       0.00      0.00      0.00        74\n",
      "         ATE       0.00      0.00      0.00        16\n",
      "         BOR       0.00      0.00      0.00         5\n",
      "         CAL       0.00      0.00      0.00        57\n",
      "         LOC       0.26      0.40      0.31        73\n",
      "         TAM       0.88      0.92      0.90        62\n",
      "\n",
      "   micro avg       0.48      0.30      0.37       287\n",
      "   macro avg       0.19      0.22      0.20       287\n",
      "weighted avg       0.25      0.30      0.27       287\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = []\n",
    "results_by_model = []\n",
    "results_by_model_by_tag = []\n",
    "\n",
    "for filename in filenames:\n",
    "    f = os.path.join(current_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        print(f)\n",
    "\n",
    "    # Transformar os dados\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "\n",
    "    # Converter as colunas para listas\n",
    "    sentences = data.groupby(\"report\").apply(to_tuples).tolist()\n",
    "    texts, tags, tags_pred = tuple_2_list(sentences)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(filename)\n",
    "    result_dict = classification_report(tags, tags_pred, mode=\"strict\", scheme=IOB2, zero_division=False)\n",
    "    print(result_dict)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    if filename == \"result_df_model_13.csv\":\n",
    "        best_result = result_dict\n",
    "        #best_model_df = pd.DataFrame.from_dict(result_dict)\n",
    "        #pd.DataFrame.from_dict(result_dict)\n",
    "        #best_model_df.to_csv(\"model_13.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 15 (char 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bilstmcrf_env/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/bilstmcrf_env/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/envs/bilstmcrf_env/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 15 (char 14)"
     ]
    }
   ],
   "source": [
    "res = json.loads(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result[-2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bilstmcrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
