{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy.util import decaying\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "from spacy.training.example import Example\n",
    "from spacy.training import biluo_tags_to_offsets\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import srsly\n",
    "import typer\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converter Formato DataFrame to Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/df_train_tokens_labeled_iob_bert_format_full.csv\", encoding=\"utf-8\")\n",
    "df_test = pd.read_csv(\"data/df_test_tokens_labeled_iob_bert_format_full.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_2_spacy_format(df):\n",
    "    spacy_format_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        entities = []\n",
    "        tokens = row['text'].split()\n",
    "        labels = row['iob_labels'].split()\n",
    "        start = 0\n",
    "        entity_label = None\n",
    "        for token, label in zip(tokens, labels):\n",
    "            end = start + len(token)\n",
    "            if label != 'O':\n",
    "                label = label.replace('B-', '').replace('I-', '').upper()\n",
    "                if label != entity_label:\n",
    "                    entity_label = label\n",
    "                    entities.append((start, end, label))\n",
    "            start = end + 1  # +1 to account for the space between tokens\n",
    "        spacy_format_data.append((text, {\"entities\": entities}))\n",
    "    return spacy_format_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spacy_format = bert_2_spacy_format(df_train)\n",
    "test_spacy_format = bert_2_spacy_format(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data, test_data = train_test_split(spacy_format_data, test_size=0.10, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(data, iterations):\n",
    "    nlp = spacy.blank(\"pt\")  \n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "    for _, annotations in data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        #dropout = decaying(0.6, 0.2, 1e-4)\n",
    "        dropout = decaying(0.2, 1e-4)\n",
    "\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            losses = {}\n",
    "            random.shuffle(data)\n",
    "            batches = minibatch(data, size=compounding(4.0, 32.0, 1.001))\n",
    "\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotation in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotation)\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, losses=losses)\n",
    "            print(\"Iteration:\", itn, \"Loss:\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "Iteration: 0 Loss: {'ner': 5018.20619669932}\n",
      "Starting iteration 1\n",
      "Iteration: 1 Loss: {'ner': 848.4883044567185}\n",
      "Starting iteration 2\n",
      "Iteration: 2 Loss: {'ner': 671.3910017149663}\n",
      "Starting iteration 3\n",
      "Iteration: 3 Loss: {'ner': 591.3945935883739}\n",
      "Starting iteration 4\n",
      "Iteration: 4 Loss: {'ner': 524.2481268858842}\n",
      "Starting iteration 5\n",
      "Iteration: 5 Loss: {'ner': 479.453393221378}\n",
      "Starting iteration 6\n",
      "Iteration: 6 Loss: {'ner': 415.8615920955134}\n",
      "Starting iteration 7\n",
      "Iteration: 7 Loss: {'ner': 376.18495290675406}\n",
      "Starting iteration 8\n",
      "Iteration: 8 Loss: {'ner': 355.44691994044007}\n",
      "Starting iteration 9\n",
      "Iteration: 9 Loss: {'ner': 345.2691531944241}\n",
      "Starting iteration 10\n",
      "Iteration: 10 Loss: {'ner': 346.1447856802871}\n",
      "Starting iteration 11\n",
      "Iteration: 11 Loss: {'ner': 314.1096225091985}\n",
      "Starting iteration 12\n",
      "Iteration: 12 Loss: {'ner': 300.77310641003055}\n",
      "Starting iteration 13\n",
      "Iteration: 13 Loss: {'ner': 268.2014066352953}\n",
      "Starting iteration 14\n",
      "Iteration: 14 Loss: {'ner': 274.95618281964187}\n",
      "Starting iteration 15\n",
      "Iteration: 15 Loss: {'ner': 275.049595617462}\n",
      "Starting iteration 16\n",
      "Iteration: 16 Loss: {'ner': 240.43355152127876}\n",
      "Starting iteration 17\n",
      "Iteration: 17 Loss: {'ner': 243.65289821760265}\n",
      "Starting iteration 18\n",
      "Iteration: 18 Loss: {'ner': 269.612097848982}\n",
      "Starting iteration 19\n",
      "Iteration: 19 Loss: {'ner': 216.71489455267906}\n",
      "Starting iteration 20\n",
      "Iteration: 20 Loss: {'ner': 228.32735513167776}\n",
      "Starting iteration 21\n",
      "Iteration: 21 Loss: {'ner': 241.83942623333968}\n",
      "Starting iteration 22\n",
      "Iteration: 22 Loss: {'ner': 243.47259296624335}\n",
      "Starting iteration 23\n",
      "Iteration: 23 Loss: {'ner': 205.25967798424065}\n",
      "Starting iteration 24\n",
      "Iteration: 24 Loss: {'ner': 200.14386907767775}\n",
      "Starting iteration 25\n",
      "Iteration: 25 Loss: {'ner': 195.94187890900247}\n",
      "Starting iteration 26\n",
      "Iteration: 26 Loss: {'ner': 204.72556674308416}\n",
      "Starting iteration 27\n",
      "Iteration: 27 Loss: {'ner': 194.58448243546198}\n",
      "Starting iteration 28\n",
      "Iteration: 28 Loss: {'ner': 192.16390803281038}\n",
      "Starting iteration 29\n",
      "Iteration: 29 Loss: {'ner': 197.3554334585103}\n",
      "Starting iteration 30\n",
      "Iteration: 30 Loss: {'ner': 191.05564470875368}\n",
      "Starting iteration 31\n",
      "Iteration: 31 Loss: {'ner': 178.77915882917605}\n",
      "Starting iteration 32\n",
      "Iteration: 32 Loss: {'ner': 191.40217728893714}\n",
      "Starting iteration 33\n",
      "Iteration: 33 Loss: {'ner': 164.01133586903353}\n",
      "Starting iteration 34\n",
      "Iteration: 34 Loss: {'ner': 196.04802305382492}\n",
      "Starting iteration 35\n",
      "Iteration: 35 Loss: {'ner': 182.10254755359284}\n",
      "Starting iteration 36\n",
      "Iteration: 36 Loss: {'ner': 161.65857263202773}\n",
      "Starting iteration 37\n",
      "Iteration: 37 Loss: {'ner': 164.25759078001963}\n",
      "Starting iteration 38\n",
      "Iteration: 38 Loss: {'ner': 162.9775918956017}\n",
      "Starting iteration 39\n",
      "Iteration: 39 Loss: {'ner': 163.60209912715413}\n",
      "Starting iteration 40\n",
      "Iteration: 40 Loss: {'ner': 165.6975779443107}\n",
      "Starting iteration 41\n",
      "Iteration: 41 Loss: {'ner': 150.96998817518144}\n",
      "Starting iteration 42\n",
      "Iteration: 42 Loss: {'ner': 163.90603030157376}\n",
      "Starting iteration 43\n",
      "Iteration: 43 Loss: {'ner': 153.56662551174355}\n",
      "Starting iteration 44\n",
      "Iteration: 44 Loss: {'ner': 158.29052789233336}\n",
      "Starting iteration 45\n",
      "Iteration: 45 Loss: {'ner': 151.4674342424207}\n",
      "Starting iteration 46\n",
      "Iteration: 46 Loss: {'ner': 142.9806788153342}\n",
      "Starting iteration 47\n",
      "Iteration: 47 Loss: {'ner': 140.34578301177672}\n",
      "Starting iteration 48\n",
      "Iteration: 48 Loss: {'ner': 149.64205964581924}\n",
      "Starting iteration 49\n",
      "Iteration: 49 Loss: {'ner': 149.81195602144436}\n",
      "Starting iteration 50\n",
      "Iteration: 50 Loss: {'ner': 141.2224972153434}\n",
      "Starting iteration 51\n",
      "Iteration: 51 Loss: {'ner': 144.44014198380168}\n",
      "Starting iteration 52\n",
      "Iteration: 52 Loss: {'ner': 128.8488402324324}\n",
      "Starting iteration 53\n",
      "Iteration: 53 Loss: {'ner': 136.04025567468562}\n",
      "Starting iteration 54\n",
      "Iteration: 54 Loss: {'ner': 129.22342237923706}\n",
      "Starting iteration 55\n",
      "Iteration: 55 Loss: {'ner': 132.48952286302753}\n",
      "Starting iteration 56\n",
      "Iteration: 56 Loss: {'ner': 131.17307600900537}\n",
      "Starting iteration 57\n",
      "Iteration: 57 Loss: {'ner': 122.00178318631761}\n",
      "Starting iteration 58\n",
      "Iteration: 58 Loss: {'ner': 120.12680646974358}\n",
      "Starting iteration 59\n",
      "Iteration: 59 Loss: {'ner': 123.24397768426097}\n",
      "Starting iteration 60\n",
      "Iteration: 60 Loss: {'ner': 132.2302710299007}\n",
      "Starting iteration 61\n",
      "Iteration: 61 Loss: {'ner': 118.80565325826426}\n",
      "Starting iteration 62\n",
      "Iteration: 62 Loss: {'ner': 121.93588226813681}\n",
      "Starting iteration 63\n",
      "Iteration: 63 Loss: {'ner': 133.4988204467935}\n",
      "Starting iteration 64\n",
      "Iteration: 64 Loss: {'ner': 120.80292566014933}\n",
      "Starting iteration 65\n",
      "Iteration: 65 Loss: {'ner': 120.7228718401317}\n",
      "Starting iteration 66\n",
      "Iteration: 66 Loss: {'ner': 135.2521330185655}\n",
      "Starting iteration 67\n",
      "Iteration: 67 Loss: {'ner': 125.56446955585326}\n",
      "Starting iteration 68\n",
      "Iteration: 68 Loss: {'ner': 127.89745469539514}\n",
      "Starting iteration 69\n",
      "Iteration: 69 Loss: {'ner': 124.65041305471176}\n",
      "Starting iteration 70\n",
      "Iteration: 70 Loss: {'ner': 112.56708208215235}\n",
      "Starting iteration 71\n",
      "Iteration: 71 Loss: {'ner': 111.58392213798224}\n",
      "Starting iteration 72\n",
      "Iteration: 72 Loss: {'ner': 105.68486202510192}\n",
      "Starting iteration 73\n",
      "Iteration: 73 Loss: {'ner': 103.58929224246647}\n",
      "Starting iteration 74\n",
      "Iteration: 74 Loss: {'ner': 124.77000586595389}\n",
      "Starting iteration 75\n",
      "Iteration: 75 Loss: {'ner': 124.55869958589241}\n",
      "Starting iteration 76\n",
      "Iteration: 76 Loss: {'ner': 109.33399913005603}\n",
      "Starting iteration 77\n",
      "Iteration: 77 Loss: {'ner': 111.84774353607783}\n",
      "Starting iteration 78\n",
      "Iteration: 78 Loss: {'ner': 102.61695042745677}\n",
      "Starting iteration 79\n",
      "Iteration: 79 Loss: {'ner': 110.13020958350856}\n",
      "Starting iteration 80\n",
      "Iteration: 80 Loss: {'ner': 107.08950651618778}\n",
      "Starting iteration 81\n",
      "Iteration: 81 Loss: {'ner': 94.68768142672151}\n",
      "Starting iteration 82\n",
      "Iteration: 82 Loss: {'ner': 105.27990076445404}\n",
      "Starting iteration 83\n",
      "Iteration: 83 Loss: {'ner': 113.57809311649567}\n",
      "Starting iteration 84\n",
      "Iteration: 84 Loss: {'ner': 93.98233266510768}\n",
      "Starting iteration 85\n",
      "Iteration: 85 Loss: {'ner': 107.42148520485374}\n",
      "Starting iteration 86\n",
      "Iteration: 86 Loss: {'ner': 104.01481430132588}\n",
      "Starting iteration 87\n",
      "Iteration: 87 Loss: {'ner': 108.74208682543697}\n",
      "Starting iteration 88\n",
      "Iteration: 88 Loss: {'ner': 92.63157024902713}\n",
      "Starting iteration 89\n",
      "Iteration: 89 Loss: {'ner': 93.10123779283708}\n",
      "Starting iteration 90\n",
      "Iteration: 90 Loss: {'ner': 96.71166817525906}\n",
      "Starting iteration 91\n",
      "Iteration: 91 Loss: {'ner': 101.64996954564394}\n",
      "Starting iteration 92\n",
      "Iteration: 92 Loss: {'ner': 102.37153068277217}\n",
      "Starting iteration 93\n",
      "Iteration: 93 Loss: {'ner': 103.900196334918}\n",
      "Starting iteration 94\n",
      "Iteration: 94 Loss: {'ner': 101.95876375940537}\n",
      "Starting iteration 95\n",
      "Iteration: 95 Loss: {'ner': 114.53788295627191}\n",
      "Starting iteration 96\n",
      "Iteration: 96 Loss: {'ner': 104.20184175385015}\n",
      "Starting iteration 97\n",
      "Iteration: 97 Loss: {'ner': 87.4043906662533}\n",
      "Starting iteration 98\n",
      "Iteration: 98 Loss: {'ner': 91.20603480176784}\n",
      "Starting iteration 99\n",
      "Iteration: 99 Loss: {'ner': 106.18911185862724}\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_spacy(train_spacy_format, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelfile = \"spacy_model_cnn\"\n",
    "#trained_model.to_disk(modelfile)\n",
    "\n",
    "modelfile = \"spacy_model_cnn_mod\"\n",
    "trained_model.to_disk(modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile_load = spacy.load('spacy_model_cnn_mod') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(modelfile, ACC):\n",
    "    nlp = modelfile\n",
    "    examples = []\n",
    "    for input_, annot in ACC:\n",
    "        #print(input)\n",
    "        doc = nlp.make_doc(input_)\n",
    "        example = Example.from_dict(doc, annot)\n",
    "        examples.append(example)\n",
    "    scorer = nlp.evaluate(examples)\n",
    "    return scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = evaluate(trained_model, test_spacy_format)\n",
    "results = evaluate(modelfile_load, test_spacy_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.834375, 'ents_r': 0.8042168674698795, 'ents_f': 0.8190184049079755, 'ents_per_type': {'ACHADO': {'p': 0.8928571428571429, 'r': 0.8823529411764706, 'f': 0.8875739644970414}, 'CALCIFICAÇÃO': {'p': 0.95, 'r': 0.9047619047619048, 'f': 0.9268292682926829}, 'LOCALIZAÇÃO': {'p': 0.8292682926829268, 'r': 0.7816091954022989, 'f': 0.8047337278106508}, 'TAMANHO': {'p': 0.7790697674418605, 'r': 0.7127659574468085, 'f': 0.7444444444444445}, 'ATENUAÇÃO': {'p': 0.7647058823529411, 'r': 0.7222222222222222, 'f': 0.7428571428571428}, 'BORDAS': {'p': 0.5454545454545454, 'r': 1.0, 'f': 0.7058823529411764}}, 'speed': 33381.680957052486}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_results = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_acc</th>\n",
       "      <th>token_p</th>\n",
       "      <th>token_r</th>\n",
       "      <th>token_f</th>\n",
       "      <th>ents_p</th>\n",
       "      <th>ents_r</th>\n",
       "      <th>ents_f</th>\n",
       "      <th>ents_per_type</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACHADO</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.834375</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.819018</td>\n",
       "      <td>{'p': 0.8928571428571429, 'r': 0.8823529411764...</td>\n",
       "      <td>33381.680957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATENUAÇÃO</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.834375</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.819018</td>\n",
       "      <td>{'p': 0.7647058823529411, 'r': 0.7222222222222...</td>\n",
       "      <td>33381.680957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BORDAS</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.834375</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.819018</td>\n",
       "      <td>{'p': 0.5454545454545454, 'r': 1.0, 'f': 0.705...</td>\n",
       "      <td>33381.680957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALCIFICAÇÃO</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.834375</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.819018</td>\n",
       "      <td>{'p': 0.95, 'r': 0.9047619047619048, 'f': 0.92...</td>\n",
       "      <td>33381.680957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCALIZAÇÃO</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.834375</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.819018</td>\n",
       "      <td>{'p': 0.8292682926829268, 'r': 0.7816091954022...</td>\n",
       "      <td>33381.680957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              token_acc  token_p  token_r  token_f    ents_p    ents_r  \\\n",
       "ACHADO              1.0      1.0      1.0      1.0  0.834375  0.804217   \n",
       "ATENUAÇÃO           1.0      1.0      1.0      1.0  0.834375  0.804217   \n",
       "BORDAS              1.0      1.0      1.0      1.0  0.834375  0.804217   \n",
       "CALCIFICAÇÃO        1.0      1.0      1.0      1.0  0.834375  0.804217   \n",
       "LOCALIZAÇÃO         1.0      1.0      1.0      1.0  0.834375  0.804217   \n",
       "\n",
       "                ents_f                                      ents_per_type  \\\n",
       "ACHADO        0.819018  {'p': 0.8928571428571429, 'r': 0.8823529411764...   \n",
       "ATENUAÇÃO     0.819018  {'p': 0.7647058823529411, 'r': 0.7222222222222...   \n",
       "BORDAS        0.819018  {'p': 0.5454545454545454, 'r': 1.0, 'f': 0.705...   \n",
       "CALCIFICAÇÃO  0.819018  {'p': 0.95, 'r': 0.9047619047619048, 'f': 0.92...   \n",
       "LOCALIZAÇÃO   0.819018  {'p': 0.8292682926829268, 'r': 0.7816091954022...   \n",
       "\n",
       "                     speed  \n",
       "ACHADO        33381.680957  \n",
       "ATENUAÇÃO     33381.680957  \n",
       "BORDAS        33381.680957  \n",
       "CALCIFICAÇÃO  33381.680957  \n",
       "LOCALIZAÇÃO   33381.680957  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy_results.to_csv(\"results_spacy_model.csv\")\n",
    "spacy_results.to_csv(\"results_spacy_model_mod.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, entities = training_data_v2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = entities['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = spacy.training.offsets_to_biluo_tags(modelfile_load.make_doc(text), entities)\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(tokens)):\n",
    "    print(\"TOKEN {}            TAG {}\".format(tokens[i], tags[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_test = spacy.blank(\"pt\")  \n",
    "ner_test = nlp_test.add_pipe(\"ner\")\n",
    "for _, annotations in training_data_v2:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        #print(ent[2])\n",
    "        ner_test.add_label(ent[2])\n",
    "other_pipes = [pipe for pipe in nlp_test.pipe_names if pipe != \"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp_test.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp_test.begin_training()\n",
    "    #dropout = decaying(0.6, 0.2, 1e-4)\n",
    "    dropout = decaying(0.2, 1e-4)\n",
    "\n",
    "    for itn in range(1):\n",
    "        print(\"Starting iteration \" + str(itn))\n",
    "        losses = {}\n",
    "        random.shuffle(training_data_v2)\n",
    "        batches = minibatch(training_data_v2, size=compounding(4.0, 32.0, 1.001))\n",
    "\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotation in batch:\n",
    "                doc = nlp_test.make_doc(text)\n",
    "                annotation = annotation[\"entities\"][0]\n",
    "                offsets_to_biluo_tags(nlp_test.make_doc(text), annotation)\n",
    "                example = Example.from_dict(doc, annotation)\n",
    "                ner_tags = example.get_aligned_ner()\n",
    "                \n",
    "                print(annotation)\n",
    "                \n",
    "\n",
    "                print(text)\n",
    "                print(doc)\n",
    "                print(ner_tags)\n",
    "\n",
    "                print(len(text.split()))\n",
    "                #print(doc.length())\n",
    "                print(len(ner_tags))\n",
    "\n",
    "                break\n",
    "                examples.append(example)\n",
    "            break\n",
    "            nlp_test.update(examples, losses=losses)\n",
    "        print(\"Iteration:\", itn, \"Loss:\", losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
